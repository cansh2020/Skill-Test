{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "89a597f0-2537-42de-9e9e-8aba3b42f9a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AnshChoudhary\\AppData\\Local\\Temp\\ipykernel_16016\\951934050.py:1692: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  ldc_error = pd.concat([ldc_error, pd.DataFrame([row])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9371.731165647507\n"
     ]
    }
   ],
   "source": [
    "import difflib # Used for sequence matching\n",
    "from fuzzywuzzy import fuzz\n",
    "import string # Used for String Operations\n",
    "import re # Used for Regex Operations\n",
    "from difflib import unified_diff\n",
    "import nltk\n",
    "import os \n",
    "import pandas as pd # Used for working on Dataframes\n",
    "import time\n",
    "import os \n",
    "import pandas as pd # Used for working on Dataframes\n",
    "from openpyxl.workbook import Workbook\n",
    "import pdfkit\n",
    "\n",
    "def find_capitalization_errors(file2_path, file1_path):\n",
    "    with open(file2_path, 'r') as file2:\n",
    "        Mtext = file2.read()\n",
    "\n",
    "    with open(file1_path, 'r') as file1:\n",
    "        Ctext = file1.read()\n",
    "\n",
    "    c = 0  # Count for substituted words\n",
    "    y = 0  # Count for misspelled words\n",
    "\n",
    "    Mtext = remove_punctuation_except_hyphen(Mtext)\n",
    "    Ctext = remove_punctuation_except_hyphen(Ctext)\n",
    "\n",
    "    MwordList = Mtext.split()\n",
    "    CwordList = Ctext.split()\n",
    "\n",
    "    master_Wtotal = len(MwordList)\n",
    "    candidate_Wtotal = len(CwordList)\n",
    "\n",
    "    differ = list(unified_diff(MwordList, CwordList))\n",
    "    segment_lenM = []\n",
    "    segment_lenC = []\n",
    "\n",
    "    for line in differ:\n",
    "        if line.startswith('@'):\n",
    "            a_match = re.search(r'-(\\d+),', line)\n",
    "            b_match = re.search(r'\\+(\\d+),', line)\n",
    "\n",
    "            a = int(a_match.group().split(',')[0]) if a_match else 0\n",
    "            b = int(b_match.group().split(',')[0]) if b_match else 0\n",
    "\n",
    "            segment_lenM.append(abs(a))\n",
    "            segment_lenC.append(b)\n",
    "\n",
    "    M_chunks = []\n",
    "    start = 0\n",
    "\n",
    "    for length in segment_lenM:\n",
    "        M_chunks.append(MwordList[start:length])\n",
    "        start = length\n",
    "\n",
    "    M_chunks.append(MwordList[start:]) # Adding the remaining elements to the last list\n",
    "\n",
    "    C_chunks = []\n",
    "    start = 0\n",
    "\n",
    "    for length in segment_lenC:\n",
    "        C_chunks.append(CwordList[start:length])\n",
    "        start = length\n",
    "\n",
    "    C_chunks.append(CwordList[start:]) # Adding the remaining elements to the last list\n",
    "\n",
    "    differ = []\n",
    "    inx = 0\n",
    "    # Compare each chunk separately\n",
    "    while inx < len(M_chunks):\n",
    "        diff = list(word_by_word_diff(C_chunks[inx], M_chunks[inx]))\n",
    "        if diff:\n",
    "            for line in diff:\n",
    "                differ.append(line)\n",
    "        inx+=1\n",
    "\n",
    "    M_index = 0\n",
    "    C_index = 0\n",
    "    t = 0\n",
    "    l = 0\n",
    "\n",
    "    # similarity = fuzz.ratio('ansh.', 'ansh,')\n",
    "    # print(similarity)\n",
    "\n",
    "    mastertext = [element for element in differ if not element.startswith('+')]\n",
    "    candidatetext = [element for element in differ if not element.startswith('-')]\n",
    "    \n",
    "    error= []\n",
    "    Capitalized = []\n",
    "    for i, item in enumerate(differ):\n",
    "        if i >= l:\n",
    "            if item.startswith('-'):  # Checks if the item in the 'differ' list represents a deletion in the master text\n",
    "                j = i\n",
    "                check1 = []\n",
    "                check2 = []\n",
    "                while j < len(differ) and not differ[j].startswith(' '):\n",
    "                    if differ[j].startswith('-'):\n",
    "                        word1 = differ[j]  # Extracts the word that is deleted in the master text\n",
    "                        # Locate word1 in the master text\n",
    "                        while M_index < len(MwordList) and not mastertext[M_index] == word1:\n",
    "                            M_index += 1  # Moves through the master text word list until it finds 'word1'\n",
    "\n",
    "                        word1 = word1[1:]\n",
    "                        check1.append((word1, M_index))\n",
    "                        M_index += 1\n",
    "                    if differ[j].startswith('+'):\n",
    "                        word2 = differ[j] # Extracts the word that is added in the candidate text\n",
    "\n",
    "                        # Locate word2 in the candidate text\n",
    "                        while C_index < len(CwordList) and not candidatetext[C_index] == word2:\n",
    "                            C_index += 1  # Moves through the candidate text word list until it finds 'word2'\n",
    "\n",
    "                        word2 = word2[1:]\n",
    "                        check2.append((word2, C_index))\n",
    "                        C_index += 1\n",
    "                    j += 1\n",
    "                    l = j\n",
    "\n",
    "                extra = 0\n",
    "                for ind, mt in enumerate(check2):\n",
    "                     element2, index2 = mt\n",
    "                     for indi, cd in enumerate(check1):\n",
    "                        if indi >= extra:\n",
    "                            element1, index1 = cd\n",
    "                            if element2 != element1 and element2.lower() == element1.lower():\n",
    "                                out = f'{{{element2}, {element1}, {index2}, {index1}, Wrong Capitalization}}'\n",
    "                                error.append([element2, element1])\n",
    "                                \n",
    "                                Capitalized.append(out)\n",
    "                                extra = indi+1\n",
    "                                \n",
    "            elif item.startswith(' '):\n",
    "                M_index += 1\n",
    "                C_index += 1\n",
    "            elif item.startswith('+'):\n",
    "                C_index += 1\n",
    "                \n",
    "    return Capitalized, error\n",
    "\n",
    "def get_word_context(words, index, context_size):\n",
    "        start = max(0, index - context_size)\n",
    "        end = min(len(words), index + context_size + 1)\n",
    "        context = ' '.join(words[start:end])\n",
    "        return context.strip()\n",
    "\n",
    "def get_aheadword_context(words, index, context_size):\n",
    "  start = min(len(words), index + 1)\n",
    "  end = min(len(words), index + context_size + 1)\n",
    "  context = ' '.join(words[start:end])\n",
    "  return context.strip()\n",
    "\n",
    "def get_behindword_context(words, index, context_size):\n",
    "  start = max(0, index - context_size)\n",
    "  end = index  # Fix: Set end to index\n",
    "  context = ' '.join(words[start:end])\n",
    "  return context.strip()\n",
    "\n",
    "def remove_punctuation_except_hyphen(text):\n",
    "    # Remove all punctuation except hyphen\n",
    "    cleaned_text = re.sub(r'[^\\w\\s-]', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_punctuation_except_few(text):\n",
    "    # Remove all punctuation except hyphen, single quote, percent sign, and full stop\n",
    "    cleaned_text = re.sub(r'[^\\w\\s.\\'%-]', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def remove_punctuation_except_fullstop(text):\n",
    "    # Remove all punctuation except hyphen\n",
    "    cleaned_text = re.sub(r'[^\\w\\s.]', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def singularize_with_spacy(word):\n",
    "    doc = nlp(word)\n",
    "    singularized_words = [token.lemma_ for token in doc]\n",
    "    return singularized_words[0] if singularized_words else word\n",
    "\n",
    "def word_by_word_diff(a, b):\n",
    "    len_a = len(a)\n",
    "    len_b = len(b)\n",
    "\n",
    "    # Initialize a matrix to track matches between words\n",
    "    matrix = [[0] * (len_b + 1) for _ in range(len_a + 1)]\n",
    "\n",
    "    # Fill the matrix to identify matching words\n",
    "    for i in range(len_a + 1):\n",
    "        for j in range(len_b + 1):\n",
    "            if i == 0 or j == 0:\n",
    "                matrix[i][j] = 0\n",
    "            elif a[i - 1] == b[j - 1]:\n",
    "                matrix[i][j] = matrix[i - 1][j - 1] + 1\n",
    "            else:\n",
    "                matrix[i][j] = max(matrix[i - 1][j], matrix[i][j - 1])\n",
    "\n",
    "    # Trace the matrix to identify differences\n",
    "    i, j = len_a, len_b\n",
    "    operations = []\n",
    "    while i > 0 or j > 0:\n",
    "        if i > 0 and j > 0 and a[i - 1] == b[j - 1]:\n",
    "            cntxt_similarity = fuzz.ratio(get_word_context(a, i-1, 1), get_word_context(b, j-1, 1))\n",
    "            if cntxt_similarity > 70:\n",
    "                operations.append(('equal', i - 1, j - 1))\n",
    "                i -= 1\n",
    "                j -= 1\n",
    "            else:\n",
    "                operations.append(('insert', j - 1))\n",
    "                j -= 1\n",
    "                operations.append(('delete', i - 1))\n",
    "                i -= 1\n",
    "        else:\n",
    "            if j > 0 and (i == 0 or matrix[i][j - 1] >= matrix[i - 1][j]):\n",
    "                operations.append(('insert', j - 1))\n",
    "                j -= 1\n",
    "            elif i > 0 and (j == 0 or matrix[i][j - 1] < matrix[i - 1][j]):\n",
    "                operations.append(('delete', i - 1))\n",
    "                i -= 1\n",
    "\n",
    "    # Reverse the operations to maintain order\n",
    "    operations.reverse()\n",
    "\n",
    "    # Generate the word list with tags\n",
    "    output = []\n",
    "    for op in operations:\n",
    "        if op[0] == 'equal':\n",
    "            output.append((f' {a[op[1]]}'))\n",
    "        elif op[0] == 'insert':\n",
    "            output.append((f'+{b[op[1]]}'))\n",
    "        elif op[0] == 'delete':\n",
    "            output.append((f'-{a[op[1]]}'))\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# Function to evaluate mistakes\n",
    "def Numberof_mistakes(Mtext, Ctext):\n",
    "\n",
    "    c = 0  # Count for substituted words\n",
    "    y = 0  # Count for misspelled words\n",
    "\n",
    "    Mtext = remove_punctuation_except_hyphen(Mtext)\n",
    "    Ctext = remove_punctuation_except_hyphen(Ctext)\n",
    "\n",
    "    MwordList = (Mtext.lower()).split()\n",
    "    CwordList = (Ctext.lower()).split()\n",
    "\n",
    "    master_Wtotal = len(MwordList)\n",
    "    candidate_Wtotal = len(CwordList)\n",
    "\n",
    "    differ = list(unified_diff(MwordList, CwordList))\n",
    "    segment_lenM = []\n",
    "    segment_lenC = []\n",
    "\n",
    "    for line in differ:\n",
    "        if line.startswith('@'):\n",
    "            a_match = re.search(r'-(\\d+),', line)\n",
    "            b_match = re.search(r'\\+(\\d+),', line)\n",
    "            \n",
    "            a = int(a_match.group().split(',')[0]) if a_match else 0\n",
    "            b = int(b_match.group().split(',')[0]) if b_match else 0\n",
    "\n",
    "            segment_lenM.append(abs(a))\n",
    "            segment_lenC.append(b)\n",
    "\n",
    "    M_chunks = []\n",
    "    start = 0\n",
    "\n",
    "    for length in segment_lenM:\n",
    "        M_chunks.append(MwordList[start:length])\n",
    "        start = length\n",
    "\n",
    "    M_chunks.append(MwordList[start:]) # Adding the remaining elements to the last list\n",
    "\n",
    "    C_chunks = []\n",
    "    start = 0\n",
    "\n",
    "    for length in segment_lenC:\n",
    "        C_chunks.append(CwordList[start:length])\n",
    "        start = length\n",
    "\n",
    "    C_chunks.append(CwordList[start:]) # Adding the remaining elements to the last list\n",
    "\n",
    "    differ = []\n",
    "\n",
    "    inx = 0\n",
    "    # Compare each chunk separately\n",
    "    while inx < len(M_chunks):\n",
    "        diff = list(word_by_word_diff(C_chunks[inx], M_chunks[inx]))\n",
    "        if diff:\n",
    "            for line in diff:\n",
    "                differ.append(line)\n",
    "        inx+=1\n",
    "\n",
    "    M_index = 0\n",
    "    C_index = 0\n",
    "    t = 0\n",
    "    tupple=[]\n",
    "    omitted=[]\n",
    "    added=[]\n",
    "    l = 0\n",
    "\n",
    "    # similarity = fuzz.ratio('ansh.', 'ansh,')\n",
    "    # print(similarity)\n",
    "\n",
    "    candidatetext = [element for element in differ if not element.startswith('+')]\n",
    "    mastertext = [element for element in differ if not element.startswith('-')]\n",
    "    # print(mastertext)\n",
    "    # print(candidatetext)\n",
    "    for i, item in enumerate(differ):\n",
    "        if i >= l:\n",
    "            if item.startswith('-'):  # Checks if the item in the 'differ' list represents a deletion in the master text\n",
    "                j = i\n",
    "                check1 = []\n",
    "                check2 = []\n",
    "                while j < len(differ) and not differ[j].startswith(' '):\n",
    "                    if differ[j].startswith('-'):\n",
    "                        word1 = differ[j]  # Extracts the word that is deleted in the master text\n",
    "                        # Locate word1 in the master text\n",
    "                        while M_index < len(MwordList) and not candidatetext[M_index] == word1:\n",
    "                            M_index += 1  # Moves through the master text word list until it finds 'word1'\n",
    "\n",
    "                        word1 = word1[1:]\n",
    "                        check1.append((word1, M_index))\n",
    "                        M_index += 1\n",
    "                    if differ[j].startswith('+'):\n",
    "                        word2 = differ[j] # Extracts the word that is added in the candidate text\n",
    "\n",
    "                        # Locate word2 in the candidate text\n",
    "                        while C_index < len(CwordList) and not mastertext[C_index] == word2:\n",
    "                            C_index += 1  # Moves through the candidate text word list until it finds 'word2'\n",
    "\n",
    "                        word2 = word2[1:]\n",
    "                        check2.append((word2, C_index))\n",
    "                        C_index += 1\n",
    "                    j += 1\n",
    "                    l = j\n",
    "                m = 0\n",
    "                # print(f\"master:{check2}\")\n",
    "                # print(f\"candi:{check1}\")\n",
    "\n",
    "                all_matched_words = []\n",
    "                replaced_words = []\n",
    "                for y, tuple2 in enumerate(check2):\n",
    "                    element2, index2 = tuple2\n",
    "                    empty = []\n",
    "                    for x, tuple1 in enumerate(check1):\n",
    "                        element1, index1 = tuple1\n",
    "                        if x>=m:\n",
    "                            similarity = fuzz.ratio(element2, element1)\n",
    "                            ahead_similarity = fuzz.ratio(get_aheadword_context(MwordList, index2, 10), get_aheadword_context(CwordList, index1, 10))\n",
    "                            behind_similarity = fuzz.ratio(get_behindword_context(MwordList, index2, 10), get_behindword_context(CwordList, index1, 10))\n",
    "                            if get_word_context(MwordList, index2, 1) and get_word_context(CwordList, index1, 1):\n",
    "                                cntxt_similarity = fuzz.ratio(get_word_context(MwordList, index2, 1), get_word_context(CwordList, index1, 1))\n",
    "                                if similarity > 60 and cntxt_similarity>60:\n",
    "                                    empty.append((((element2, index2), (element1, index1)), similarity, x))\n",
    "                                elif similarity == 100 and (ahead_similarity>70 or behind_similarity>70):\n",
    "                                    empty.append((((element2, index2), (element1, index1)), similarity, x))\n",
    "                                elif similarity > 65 and (ahead_similarity>70 or behind_similarity>70):\n",
    "                                    empty.append((((element2, index2), (element1, index1)), similarity, x))\n",
    "\n",
    "                    sorted_empty = sorted(empty, key=lambda x: x[1], reverse=True)  # Sort based on the second element of each tuple\n",
    "                    if len(sorted_empty)>0:\n",
    "                        # tupple.append(sorted_empty[0][0])\n",
    "                        all_matched_words.append((sorted_empty[0][0], y, sorted_empty[0][2]))\n",
    "                        replaced_words.append((y, sorted_empty[0][2]))\n",
    "                        m = sorted_empty[0][2] + 1\n",
    "\n",
    "                for i, x in enumerate(all_matched_words):\n",
    "                    if (i+1) in range(len(all_matched_words)):\n",
    "                        midx1 = x[1]\n",
    "                        cidx1 = x[2]\n",
    "                        midx2 = all_matched_words[i+1][1]\n",
    "                        cidx2 = all_matched_words[i+1][2]\n",
    "\n",
    "                        if (midx2 - midx1) > 0 and (cidx2 - cidx1) > 0:\n",
    "                            mrange = [m for m in range((midx1 + 1), midx2)]\n",
    "                            crange = [n for n in range((cidx1 + 1), cidx2)]\n",
    "\n",
    "                            # Pair elements from mrange and crange\n",
    "                            paired_elements = list(zip(mrange, crange))\n",
    "                            for pair in paired_elements:\n",
    "                                if len(pair)>0:\n",
    "                                    replaced_words.append(pair)\n",
    "                    else:\n",
    "                        midx1 = x[1]\n",
    "                        cidx1 = x[2]\n",
    "\n",
    "                        if (len(check2) -1 -midx1) > 0 and (len(check1) -1 - cidx1) > 0:\n",
    "                            mrange = [m for m in range((midx1 + 1), (len(check2)))]\n",
    "                            crange = [n for n in range((cidx1 + 1), (len(check1)))]\n",
    "\n",
    "                            # Pair elements from mrange and crange\n",
    "                            paired_elements = list(zip(mrange, crange))\n",
    "                            for pair in paired_elements:\n",
    "                                if len(pair)>0:\n",
    "                                    replaced_words.append(pair)\n",
    "                if len(all_matched_words)>0:\n",
    "                    if all_matched_words[0][1]>0 and all_matched_words[0][2]>0:\n",
    "                        midx2 = all_matched_words[0][1]\n",
    "                        cidx2 = all_matched_words[0][2]\n",
    "\n",
    "                        if (midx2) > 0 and (cidx2) > 0:\n",
    "                            mrange = [m for m in range(0, midx2)]\n",
    "                            crange = [n for n in range(0, cidx2)]\n",
    "                            \n",
    "                            # Pair elements from mrange and crange\n",
    "                            paired_elements = list(zip(mrange, crange))\n",
    "                            for pair in paired_elements:\n",
    "                                if len(pair)>0:\n",
    "                                    replaced_words.append(pair)\n",
    "\n",
    "                if len(all_matched_words)==0:\n",
    "                    mrange = [m for m in range(0, len(check2))]\n",
    "                    crange = [n for n in range(0, len(check1))]\n",
    "                    # Pair elements from mrange and crange\n",
    "                    paired_elements = list(zip(mrange, crange))\n",
    "                    for pair in paired_elements:\n",
    "                        if len(pair)>0:\n",
    "                            replaced_words.append(pair)\n",
    "\n",
    "                replaced_words = sorted(replaced_words, key=lambda x:x[0])\n",
    "                for one in replaced_words:\n",
    "                    a1, a2 = one\n",
    "                    tupple.append((check2[a1], check1[a2]))\n",
    "                    # print(f'match: {(check2[a1], check1[a2])}')\n",
    "                empty = []\n",
    "                if len(check2)>0:\n",
    "                    for om_i, om_tup in enumerate(check2):\n",
    "                        for tu in replaced_words:\n",
    "                            mi = tu[0]\n",
    "                            empty.append(mi)\n",
    "\n",
    "                        if om_i not in empty:\n",
    "                            omitted.append(om_tup)\n",
    "\n",
    "                empty = []\n",
    "                if len(check1)>0:\n",
    "                    for ad_i, ad_tup in enumerate(check1):\n",
    "                        for tu in replaced_words:\n",
    "                            ci = tu[1]\n",
    "                            empty.append(ci)\n",
    "\n",
    "                        if ad_i not in empty:\n",
    "                            added.append(ad_tup)\n",
    "\n",
    "            elif item.startswith(' '):\n",
    "                M_index += 1\n",
    "                C_index += 1\n",
    "            elif item.startswith('+'):\n",
    "                omitted.append((item[1:], C_index))\n",
    "                C_index += 1\n",
    "                \n",
    "    a = len(omitted)  # Omitted words count\n",
    "    b = len(added)  # Added words count\n",
    "    y = 0\n",
    "    Mispelled_List = []  # List to hold misspelled words\n",
    "    Substitute = []  # List to hold substituted \n",
    "    for inx, tup in enumerate(tupple):\n",
    "        Mw, Cw = tup\n",
    "        if Mw[0] != Cw[0]:\n",
    "            similarity = fuzz.ratio(Mw[0], Cw[0])\n",
    "            if similarity > 65 :#and (min((len(Mw[0])), len(Cw[0]))/max(len(Mw[0]), len(Cw[0])))> 0.5:  # Check if the candidate word is a misspelling of the master word\n",
    "                Mispelled_List.append(tup)  # Collect misspelled words\n",
    "                y+=1\n",
    "            else:\n",
    "                c += 1\n",
    "                Substitute.append(tup)  # Collect wrongly substituted words\n",
    "\n",
    "    for i, tup in enumerate(Mispelled_List):\n",
    "        p, q = tup\n",
    "        p1, p2 = p\n",
    "        q1, q2 = q\n",
    "        Mispelled_List[i] = (p1, q1, p2, q2)\n",
    "\n",
    "    for i, tup in enumerate(Substitute):\n",
    "        p, q = tup\n",
    "        p1, p2 = p\n",
    "        q1, q2 = q\n",
    "        Substitute[i] = (p1, q1, p2, q2)\n",
    "\n",
    "    return a, b, c, y, omitted, added, Mispelled_List, Substitute\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def find_full_capitalization_errors(file1_path, file2_path):\n",
    "    cap_error = []\n",
    "    with open(file1_path, 'r') as file1, open(file2_path, 'r') as file2:\n",
    "        content1 = file1.read()\n",
    "        content2 = file2.read()\n",
    "        \n",
    "    errors = [] \n",
    "    output1 = ''\n",
    "    matched_words = set()\n",
    "    words_file1 = re.findall(r'\\b\\w+\\b', content1)\n",
    "    words_file2 = re.findall(r'\\b\\w+\\b', content2)\n",
    "\n",
    "    position_file1 = 0\n",
    "    position_file2 = 0\n",
    "\n",
    "    seq_matcher = difflib.SequenceMatcher(None, words_file1, words_file2)\n",
    "    opcodes = seq_matcher.get_opcodes()\n",
    "\n",
    "    for opcode, i1, i2, j1, j2 in opcodes:\n",
    "        if opcode == \"equal\":\n",
    "            position_file1 += i2 - i1\n",
    "            position_file2 += j2 - j1\n",
    "            continue\n",
    "\n",
    "        for word1 in words_file1[i1:i2]:\n",
    "            for word2 in words_file2[j1:j2]:\n",
    "                close_matches = difflib.get_close_matches(word1.lower(), [word2.lower()], n=1, cutoff=0.8)\n",
    "                if close_matches:\n",
    "                    if word1.isupper() and word1.lower() == close_matches[0].lower():\n",
    "                        error1 = word1, word2, position_file1, position_file2, 'Full Capitalization Error'\n",
    "                        error2 = f\"{{{word2}, {word1}, {position_file2}, {position_file1}, 'Full Capitalization Error'}};\"\n",
    "                        cap_error.append(error1)\n",
    "                        errors.append(error2)\n",
    "                        output1 = ' '.join(errors)\n",
    "                        matched_words.add(word1)\n",
    "            \n",
    "                    break  \n",
    "            position_file2 += 1  \n",
    "            position_file1 += 1\n",
    "        \n",
    "        position_file1 += i2 - i1\n",
    "        position_file2 += j2- j1\n",
    "\n",
    "        position_file2 = j1\n",
    "        position_file1 = i1\n",
    "\n",
    "    return errors, cap_error\n",
    "\n",
    "def Splconandtrans(file2_path, file1_path):\n",
    "    with open(file2_path, 'r') as file2:\n",
    "        Mtext = file2.read()\n",
    "\n",
    "    with open(file1_path, 'r') as file1:\n",
    "        Ctext = file1.read()\n",
    "\n",
    "    c = 0  # Count for substituted words\n",
    "    y = 0  # Count for misspelled words\n",
    "\n",
    "    Mtext = remove_punctuation_except_hyphen(Mtext)\n",
    "    Ctext = remove_punctuation_except_hyphen(Ctext)\n",
    "\n",
    "    MwordList = (Mtext.lower()).split()\n",
    "    CwordList = (Ctext.lower()).split()\n",
    "\n",
    "    master_Wtotal = len(MwordList)\n",
    "    candidate_Wtotal = len(CwordList)\n",
    "\n",
    "    differ = list(unified_diff(MwordList, CwordList))\n",
    "    segment_lenM = []\n",
    "    segment_lenC = []\n",
    "\n",
    "    for line in differ:\n",
    "        if line.startswith('@'):\n",
    "            a_match = re.search(r'-(\\d+),', line)\n",
    "            b_match = re.search(r'\\+(\\d+),', line)\n",
    "            \n",
    "            a = int(a_match.group().split(',')[0]) if a_match else 0\n",
    "            b = int(b_match.group().split(',')[0]) if b_match else 0\n",
    "\n",
    "            segment_lenM.append(abs(a))\n",
    "            segment_lenC.append(b)\n",
    "\n",
    "    M_chunks = []\n",
    "    start = 0\n",
    "\n",
    "    for length in segment_lenM:\n",
    "        M_chunks.append(MwordList[start:length])\n",
    "        start = length\n",
    "\n",
    "    M_chunks.append(MwordList[start:]) # Adding the remaining elements to the last list\n",
    "\n",
    "    C_chunks = []\n",
    "    start = 0\n",
    "\n",
    "    for length in segment_lenC:\n",
    "        C_chunks.append(CwordList[start:length])\n",
    "        start = length\n",
    "\n",
    "    C_chunks.append(CwordList[start:]) # Adding the remaining elements to the last list\n",
    "\n",
    "    differ = []\n",
    "\n",
    "    inx = 0\n",
    "    # Compare each chunk separately\n",
    "    while inx < len(M_chunks):\n",
    "        diff = list(word_by_word_diff(C_chunks[inx], M_chunks[inx]))\n",
    "        if diff:\n",
    "            for line in diff:\n",
    "                differ.append(line)\n",
    "        inx+=1\n",
    "\n",
    "    M_index = 0\n",
    "    C_index = 0\n",
    "    t = 0\n",
    "    tupple=[]\n",
    "    omitted=[]\n",
    "    added=[]\n",
    "    l = 0\n",
    "\n",
    "    # similarity = fuzz.ratio('ansh.', 'ansh,')\n",
    "    # print(similarity)\n",
    "\n",
    "    mastertext = [element for element in differ if not element.startswith('+')]\n",
    "    candidatetext = [element for element in differ if not element.startswith('-')]\n",
    "\n",
    "    splits = []\n",
    "    concat = []\n",
    "    transposed = []\n",
    "    for i, item in enumerate(differ):\n",
    "        if i >= l:\n",
    "            if item.startswith('-'):  # Checks if the item in the 'differ' list represents a deletion in the master text\n",
    "                j = i\n",
    "                check1 = []\n",
    "                check2 = []\n",
    "                while j < len(differ) and not differ[j].startswith(' '):\n",
    "                    if differ[j].startswith('-'):\n",
    "                        word1 = differ[j]  # Extracts the word that is deleted in the master text\n",
    "                        # Locate word1 in the master text\n",
    "                        while M_index < len(MwordList) and not mastertext[M_index] == word1:\n",
    "                            M_index += 1  # Moves through the master text word list until it finds 'word1'\n",
    "\n",
    "                        word1 = word1[1:]\n",
    "                        check1.append((word1, M_index))\n",
    "                        M_index += 1\n",
    "                    if differ[j].startswith('+'):\n",
    "                        word2 = differ[j] # Extracts the word that is added in the candidate text\n",
    "\n",
    "                        # Locate word2 in the candidate text\n",
    "                        while C_index < len(CwordList) and not candidatetext[C_index] == word2:\n",
    "                            C_index += 1  # Moves through the candidate text word list until it finds 'word2'\n",
    "\n",
    "                        word2 = word2[1:]\n",
    "                        check2.append((word2, C_index))\n",
    "                        C_index += 1\n",
    "                    j += 1\n",
    "                    l = j\n",
    "                # print(check2)\n",
    "                # print(check1)\n",
    "                extra = 0\n",
    "                for ind, mt in enumerate(check2):\n",
    "                     element2, index2 = mt\n",
    "                     for indi, cd in enumerate(check1):\n",
    "                        if indi >= extra:\n",
    "                            element1n1, index1n1 = cd\n",
    "                            if (indi+1)<len(check1):\n",
    "                                    element1n2, index1n2 = check1[indi+1]\n",
    "                                    if element2 == f'{element1n1}{element1n2}':\n",
    "                                        out = f'{{{element2}, {element1n1} {element1n2}, {index2}, {index1n1}, Splitted Word}}'\n",
    "                                        splits.append(out)\n",
    "                                        extra = indi+1\n",
    "                                    elif element2 == f'{element1n1}-{element1n2}':\n",
    "                                        out = f'{{{element2}, {element1n1} {element1n2}, {index2}, {index1n1}, Splitted Word}}'\n",
    "                                        splits.append(out)\n",
    "                                        extra = indi+1\n",
    "\n",
    "                extra = 0\n",
    "                for ind, mt in enumerate(check1):\n",
    "                     element1, index1 = mt\n",
    "                     for indi, cd in enumerate(check2):\n",
    "                        if indi >= extra:\n",
    "                            element2n1, index2n1 = cd\n",
    "                            if (indi+1)<len(check2):\n",
    "                                    element2n2, index2n2 = check2[indi+1]\n",
    "                                    if element1 == f'{element2n1}{element2n2}':\n",
    "                                        out = f'{{{element2n1} {element2n2}, {element1}, {index2n1}, {index1}, concatnated Word}}'\n",
    "                                        concat.append(out)\n",
    "                                        extra = indi+1\n",
    "                                    elif element1 == f'{element2n1}-{element2n2}':\n",
    "                                        out = f'{{{element2n1} {element2n2}, {element1}, {index2n1}, {index1}, concatnated Word}}'\n",
    "                                        concat.append(out)\n",
    "                                        extra = indi+1\n",
    "                \n",
    "                extra = 0\n",
    "                for ind, mt in enumerate(check2):\n",
    "                    element2n1, index2n1 = mt\n",
    "                    if (ind+1)<len(check2):\n",
    "                        element2n2, index2n2 = check2[ind+1]\n",
    "                        for indi, cd in enumerate(check1):\n",
    "                            if indi >= extra:\n",
    "                                element1n1, index1n1 = cd\n",
    "                                if (indi+1)<len(check1):\n",
    "                                        element1n2, index1n2 = check1[indi+1]\n",
    "                                        if element2n1 == element1n2 and element2n2 == element1n1:\n",
    "                                            out = f'{{{element2n1} {element2n2}, {element1n1} {element1n2}, {index2n1}, {index1n1}, Transposed Word}}'\n",
    "                                            transposed.append(out)\n",
    "                                            extra = indi+1\n",
    "            elif item.startswith(' '):\n",
    "                M_index += 1\n",
    "                C_index += 1\n",
    "            elif item.startswith('+'):\n",
    "                added.append((item[1:], C_index))\n",
    "                C_index += 1\n",
    "                \n",
    "    return splits, concat, transposed\n",
    "\n",
    "def find_fullstop_errors(file1_path, file2_path):\n",
    "    errors = []\n",
    "\n",
    "    def count_fullstops(word):\n",
    "        return word.count('.')\n",
    "\n",
    "    def count_indices(word_file):\n",
    "        with open(word_file, 'r', encoding='latin-1') as file:\n",
    "            content = file.read()\n",
    "        words_file = content.split()\n",
    "        return words_file\n",
    "\n",
    "    word_file1 = count_indices(file1_path)\n",
    "    word_file2 = count_indices(file2_path)\n",
    "    \n",
    "    def find_word_indices(word_file):\n",
    "        word_indices = {}\n",
    "        for i, word in enumerate(word_file):\n",
    "            if word.endswith('.'):\n",
    "                word_lower = word.lower()\n",
    "                if word_lower not in word_indices:\n",
    "                    word_indices[word_lower] = []\n",
    "                word_indices[word_lower].append(i)\n",
    "        return word_indices\n",
    "\n",
    "    word_indices1 = find_word_indices(word_file1)\n",
    "    word_indices2 = find_word_indices(word_file2)\n",
    "    \n",
    "    all_valuesM = [value for sublist in word_indices2.values() for value in sublist]\n",
    "    all_valuesC = [value for sublist in word_indices1.values() for value in sublist]\n",
    "    all_valuesM = sorted(all_valuesM)\n",
    "    all_valuesC = sorted(all_valuesC)\n",
    "    master_found = []\n",
    "    candi_found = []\n",
    "    extra = 0\n",
    "    \n",
    "    for i2, value2 in enumerate(all_valuesM):\n",
    "        for i1, value1 in enumerate(all_valuesC):\n",
    "            if i1>=extra:\n",
    "                similarityB = fuzz.ratio(get_behindword_context(word_file2, value2, 8), get_behindword_context(word_file1, value1, 8))\n",
    "                similarityA = fuzz.ratio(get_aheadword_context(word_file2, value2, 8), get_aheadword_context(word_file1, value1, 8))\n",
    "                if similarityB > 75 or similarityA > 75:\n",
    "                    master_found.append(value2)\n",
    "                    candi_found.append(value1)\n",
    "                    extra = i1 + 1\n",
    "                    break\n",
    "\n",
    "    for i in candi_found:\n",
    "        result = count_fullstops(word_file1[i])\n",
    "        error_info = [word_file1[i], '99999', i, 'unnecessary fullstop']\n",
    "        if result > 1:\n",
    "            error_info = [word_file1[i], '99999', i, 'unnecessary fullstop']\n",
    "            errors.extend([error_info] * (result-1))\n",
    "        \n",
    "    indices2 = [value for value in all_valuesM if value not in master_found]\n",
    "    indices1 = [value for value in all_valuesC if value not in candi_found]\n",
    "    \n",
    "    for i in indices1:\n",
    "        result = count_fullstops(word_file1[i])\n",
    "        error_info = [word_file1[i], '99999', i, 'unnecessary fullstop']\n",
    "        if result> 1:\n",
    "            errors.extend([error_info] * (result-1))\n",
    "        else:\n",
    "            errors.extend([error_info])\n",
    "    for i in indices2:\n",
    "        errors.extend([[word_file2[i], i, '99999', 'missing fullstop']])\n",
    "        \n",
    "    idx = []    \n",
    "    for i in errors:\n",
    "        if i[3].startswith('miss'):\n",
    "            idx.append(i[1])\n",
    "        elif i[3].startswith('unn'):\n",
    "            idx.append(i[2])\n",
    "        idx = sorted(idx)\n",
    "        \n",
    "    sorted_data = sorted(errors, key=lambda x: idx.index(x[1]) if x[3] == 'missing fullstop' else idx.index(x[2]))\n",
    "    sorted_data_str = \"{\" + \"}, {\".join(\", \".join(str(i) for i in item) for item in sorted_data) + \"}\"\n",
    "    \n",
    "    return sorted_data, sorted_data_str\n",
    "\n",
    "def calculate_mistake_percentage(file1_path, file2_path, ofile1_pth, ofile2_pth):\n",
    "    def count_words(file_path):\n",
    "        with open(file_path, 'r') as words_file:\n",
    "            words_text = words_file.read()\n",
    "            words_list = words_text.split()\n",
    "            \n",
    "        return len(words_list)\n",
    "\n",
    "    def roll_no(file_path):\n",
    "        file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        return file_name\n",
    "\n",
    "    total_words = 0\n",
    "    full_mistakes = 0\n",
    "    half_mistakes = 0\n",
    "\n",
    "    result5, out1, result4 = Splconandtrans(file2_path, file1_path)\n",
    "    if out1:\n",
    "        half_mistakes += len(out1)\n",
    "    # print(out1)\n",
    "    # print()\n",
    "    \n",
    "    if result5:\n",
    "        half_mistakes += len(result5)  \n",
    "    # print(result5)\n",
    "    # print()\n",
    "        \n",
    "    if result4:\n",
    "        half_mistakes += len(result4)\n",
    "    # print(result4)\n",
    "    # print()\n",
    "    \n",
    "    result3,cap_err = find_capitalization_errors(file2_path, file1_path)\n",
    "    if result3:\n",
    "        half_mistakes += len(cap_err)\n",
    "    # print(result3)\n",
    "    # print()\n",
    "\n",
    "    with open(file2_path, 'r') as mfile:\n",
    "        master_text = mfile.read()\n",
    "    with open(file1_path, 'r') as cfile:\n",
    "        candidate_text = cfile.read()\n",
    "    \n",
    "    candidate_text = remove_punctuation_except_hyphen(candidate_text)\n",
    "    splitted_ctext = (candidate_text.lower()).split()\n",
    "    words_to_replace = []\n",
    "    for x in out1:\n",
    "        xlist = x.split(', ')\n",
    "        splitted_ctext[int(xlist[-2])] = xlist[0][1:]\n",
    "\n",
    "    for x in result4:\n",
    "        xlist = x.split(', ')\n",
    "        words = xlist[0].split()\n",
    "        splitted_ctext[int(xlist[-2])] = words[0][1:]\n",
    "        splitted_ctext[int(xlist[-2]) + 1] = words[1]\n",
    "        \n",
    "    indexes_to_remove = []\n",
    "    for x in result5:\n",
    "        xlist = x.split(', ')\n",
    "        splitted_ctext[int(xlist[-2])] = xlist[0][1:]\n",
    "        indexes_to_remove.append(int(xlist[-2])+1)\n",
    "        \n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        splitted_ctext.pop(idx)\n",
    "\n",
    "    candidate_text = ' '.join(splitted_ctext)\n",
    "    candidate_text = candidate_text.strip()\n",
    "    candidate_text = candidate_text.lower()\n",
    "    master_text = remove_punctuation_except_hyphen(master_text)\n",
    "    master_text = master_text.lower()\n",
    "\n",
    "    a, b, c, z, missed_words, added_words, misspelled, replaced_words = Numberof_mistakes(master_text, candidate_text)\n",
    "\n",
    "    final_spelling_mistakes = ''\n",
    "    total_spelling_mistakes = 0 \n",
    "    for mstk in misspelled:\n",
    "        final_spelling_mistakes=final_spelling_mistakes+'{'+mstk[0]+', '+mstk[1]+', '+str(mstk[2])+', '+str(mstk[3])+', Spelling Mistake}; '\n",
    "        total_spelling_mistakes+=1\n",
    "    errors = []\n",
    "    \n",
    "    total_replaced_words = 0 \n",
    "    for repl_wrds in replaced_words:\n",
    "        errors.extend([[repl_wrds[0], repl_wrds[1], repl_wrds[2], repl_wrds[3], 'Replaced Word']])\n",
    "        total_replaced_words+=1\n",
    "\n",
    "    #FINAL MISSED WORDS ARE STORED IN BELOW LIST\n",
    "    total_missed_words_count = 0\n",
    "    for msd_wrds in missed_words:\n",
    "        errors.extend([[msd_wrds[0], msd_wrds[1], '99999', 'Missed']])\n",
    "        total_missed_words_count+=1\n",
    "        \n",
    "    #Filter those extra added words which is already taken in spelling mistakes and replace words mistake\n",
    "    total_count_of_added_words = 0\n",
    "    for extr_add_wrd in added_words:\n",
    "        errors.extend([[extr_add_wrd[0], '99999', extr_add_wrd[1], 'Added']])\n",
    "        total_count_of_added_words+=1\n",
    "        \n",
    "    idx = []\n",
    "    \n",
    "    for i in errors:\n",
    "        # print(i)\n",
    "        if len(i)==4:\n",
    "            if 'Missed' in i[3]:\n",
    "                idx.append(i[1])\n",
    "            elif 'Added' in i[3]:\n",
    "                idx.append(i[2])\n",
    "        else:\n",
    "            idx.append(i[2])\n",
    "        idx = sorted(idx)\n",
    "        \n",
    "    sorted_data_full = []\n",
    "    for error in errors:\n",
    "        if len(error)==4:\n",
    "            if error[3] == 'Missed':\n",
    "                sorted_data_full.append((error, idx.index(error[1])))\n",
    "            elif error[3] == 'Added':\n",
    "                sorted_data_full.append((error, idx.index(error[2])))\n",
    "        else:\n",
    "            sorted_data_full.append((error, idx.index(error[2])))\n",
    "    \n",
    "    sorted_data_full.sort(key=lambda x: x[1])\n",
    "    sorted_data_full = [item[0] for item in sorted_data_full]\n",
    "    sorted_data_full_err = \"{\" + \"}, {\".join(\", \".join(str(i) for i in item) for item in sorted_data_full) + \"}\"\n",
    "\n",
    "    if replaced_words:\n",
    "        full_mistakes +=  total_replaced_words\n",
    "    # print(replaced_words)\n",
    "    # print(total_replaced_words)\n",
    "    # print()\n",
    "\n",
    "    if missed_words:\n",
    "        full_mistakes += total_missed_words_count\n",
    "    # print(missed_words)\n",
    "    # print(total_missed_words_count)\n",
    "    # print()\n",
    "\n",
    "    if added_words:\n",
    "        full_mistakes += total_count_of_added_words\n",
    "    # print(added_words)\n",
    "    # print(total_count_of_added_words)\n",
    "    # print()\n",
    "        \n",
    "    H_ERROR_CNT = half_mistakes\n",
    "    F_ERROR_CNT = full_mistakes\n",
    "    \n",
    "    \n",
    "    if misspelled:\n",
    "        half_mistakes += total_spelling_mistakes\n",
    "    # print(misspelled)\n",
    "    # print(total_spelling_mistakes)\n",
    "    # print()\n",
    "\n",
    "    sorted_data, result8 = find_fullstop_errors(file1_path, file2_path)\n",
    "    if sorted_data:\n",
    "        half_mistakes += len(sorted_data)\n",
    "    # print(result8)\n",
    "    # print()\n",
    "\n",
    "    result9,cap_err = find_full_capitalization_errors(file1_path, file2_path)\n",
    "    # if result9:\n",
    "        # full_mistakes += len(result9)\n",
    "    # print(result9)\n",
    "    # print()\n",
    "\n",
    "    with open(file2_path, 'r') as file2:\n",
    "        words_file2 = file2.read().lower().split()\n",
    "        words_file2 = [word.strip(string.punctuation) for word in words_file2]\n",
    "        total_words = len(words_file2)\n",
    "\n",
    "\n",
    "    punctuation_set = set(string.punctuation)\n",
    "    punctuation_count = 0\n",
    "    tab_count = 0\n",
    "\n",
    "    with open(file2_path, 'r') as file:\n",
    "        content = file.read()\n",
    "        for char in content:\n",
    "            if char in punctuation_set:\n",
    "                punctuation_count += 1\n",
    "\n",
    "        for line in file:\n",
    "            tab_count += line.count('\\t')\n",
    "                    \n",
    "    total_words_master = total_words + punctuation_count + tab_count\n",
    "    total_mistakes = (half_mistakes / 2) + full_mistakes \n",
    "    mistake_percentage = ((total_mistakes * 100) / 800)\n",
    "    \n",
    "    if mistake_percentage > 100:\n",
    "        mistake_percentage = 100\n",
    "    # print(mistake_percentage)\n",
    "    # print()\n",
    "\n",
    "    path_to_wkhtmltopdf = r'C:\\Program Files\\wkhtmltopdf\\bin\\wkhtmltopdf.exe'\n",
    "\n",
    "    config = pdfkit.configuration(wkhtmltopdf=path_to_wkhtmltopdf)\n",
    "\n",
    "    # Define your result variables\n",
    "    missing_space = out1\n",
    "    capitalization_errors = result3\n",
    "    transposed_errors = result4\n",
    "    splitted_words = result5\n",
    "    misspelled_words = final_spelling_mistakes\n",
    "    combined_full = sorted_data_full_err\n",
    "    fullstop_err = result8\n",
    "    fullcapitalization_err = result9\n",
    "    roll_no = roll_no(file1_path)\n",
    "    words_count1 = count_words(ofile1_pth)\n",
    "\n",
    "    # Function to format error details as a string\n",
    "    def format_error_details(error_list):\n",
    "        if len(error_list)>0:\n",
    "            return ''.join(error_list)\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "    # Categorize errors\n",
    "    spelling_mistake = format_error_details(misspelled_words)\n",
    "    other_than_spelling_mistake = format_error_details(combined_full)\n",
    "\n",
    "    # Calculate the counts for the other error types\n",
    "    spacing_cap_transp_mistake = format_error_details(capitalization_errors)  + format_error_details(result4) + format_error_details(out1) + format_error_details(splitted_words)\n",
    "    punctuation_error = format_error_details(fullstop_err)\n",
    "\n",
    "    html_string = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "    <style>\n",
    "    table, th, td {{\n",
    "        border: 1px solid black;\n",
    "        border-collapse: collapse;\n",
    "        font-size: 18px;\n",
    "    }}\n",
    "    </style>\n",
    "    </head>\n",
    "    <body  style=\"padding:15px;\">\n",
    "    <div style=\"text-align:center;\">\n",
    "    <h2 style=\"font-size: 32px;\">Staff Selection Commission</h2>\n",
    "    <h2 style=\"font-size: 32px;\">Typing Test Candidate Report</h2>\n",
    "    </div>\n",
    "\n",
    "    <div>\n",
    "    <p style=\"font-size: 20px;\"><b> Name:****** </b></p>\n",
    "    <p style=\"font-size: 20px;\"><b> Roll Number: {roll_no} </b></p>\n",
    "    <p style=\"font-size: 20px;\"><b> No. of words typed: {words_count1} </b></p>\n",
    "    </div>\n",
    "\n",
    "    <table style=\"width:83%\">\n",
    "    <tr> \n",
    "        <th colspan=\"2\" style=\"background-color: LightSteelBlue;\">Type of Mistakes</th>\n",
    "        <th style=\"background-color: LightSteelBlue;\">No. of Error</th>\n",
    "        <th style=\"background-color: LightSteelBlue;\">Error Detail</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=\"1\"><strong> Full Mistake </strong></td>\n",
    "        <td>Other than Spelling Mistake (Omission/Substitution except transposition/ Addition/ Incomplete Word) </td>\n",
    "        <td> {(total_replaced_words) + (total_missed_words_count) + (total_count_of_added_words)} </td>\n",
    "        <td>{other_than_spelling_mistake}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=\"3\"><strong> Half Mistake </strong></td>\n",
    "        <td>Splitted/Concatnated/Wrong Capitalization/Transposition Mistake</td>\n",
    "        <td>{len(capitalization_errors) + len(out1) + len(result4) + len(splitted_words)}</td>\n",
    "        <td>{spacing_cap_transp_mistake}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Spelling Mistake</td>\n",
    "        <td>{total_spelling_mistakes}</td>\n",
    "        <td>{spelling_mistake}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Punctuation Error</td>\n",
    "        <td>{len(sorted_data)}</td>\n",
    "        <td>{fullstop_err}</td>\n",
    "    </tr>\n",
    "    </table>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    pdf_output_path = r\"C:\\Users\\AnshChoudhary\\Downloads\\Skilltest-2024\\Skilltest-2024\\Evaluated\\4th Jan- S1\\PDF_Evaluated 4th Jan- S1\\{}.pdf\".format(roll_no)\n",
    "\n",
    "    options = {\n",
    "        'page-size': 'A4',\n",
    "        'margin-top': '10mm',\n",
    "        'margin-right': '0mm',\n",
    "        'margin-bottom': '10mm',\n",
    "        'margin-left': '0mm',\n",
    "    }\n",
    "\n",
    "    pdfkit.from_string(html_string, pdf_output_path, configuration=config, options=options)\n",
    "\n",
    "    # print(\"PDF saved at:\", pdf_output_path)\n",
    "    # print(\"Total time taken:- \")\n",
    "    \n",
    "    return total_spelling_mistakes, H_ERROR_CNT, F_ERROR_CNT, total_mistakes, total_words, mistake_percentage, len(sorted_data)\n",
    "\n",
    "def candidate_replacement(file1_path):\n",
    "    with open(file1_path, 'r') as file1:\n",
    "        candidate_text = file1.read()\n",
    "    candidate_text = remove_punctuation_except_few(candidate_text)\n",
    "    CwordList = candidate_text.split()\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'Hon\\'ble':\n",
    "            CwordList[idx] = 'honourable'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'Hon\\'ble.':\n",
    "            CwordList[idx] = 'honourable.'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'hon\\'ble':\n",
    "            CwordList[idx] = 'honourable'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'hon\\'ble.':\n",
    "            CwordList[idx] = 'honourable.'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'Hon.':\n",
    "            CwordList[idx] = 'honourable'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'hon.':\n",
    "            CwordList[idx] = 'honourable'\n",
    "\n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'agriculture' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'based':\n",
    "            CwordList[idx] = 'agriculture-based'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "\n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'agriculture' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'based.':\n",
    "            CwordList[idx] = 'agriculture-based.'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'Honourable':\n",
    "            CwordList[idx] = 'honourable'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'Honourable.':\n",
    "            CwordList[idx] = 'honourable.'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '70':\n",
    "            CwordList[idx] = 'seventy'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '70.':\n",
    "            CwordList[idx] = 'seventy.'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'per-cent':\n",
    "            CwordList[idx] = 'per cent'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'per-cent.':\n",
    "            CwordList[idx] = 'per cent.'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '%':\n",
    "            CwordList[idx] = 'per cent'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '%.':\n",
    "            CwordList[idx] = 'per cent.'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '70%':\n",
    "            CwordList[idx] = 'seventy per cent'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '70%.':\n",
    "            CwordList[idx] = 'seventy per cent.'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '2':\n",
    "            CwordList[idx] = 'two'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '2.':\n",
    "            CwordList[idx] = 'two.'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'industrialised':\n",
    "            CwordList[idx] = 'industrialized'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'industrialised.':\n",
    "            CwordList[idx] = 'industrialized.'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '4':\n",
    "            CwordList[idx] = 'four'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '4.':\n",
    "            CwordList[idx] = 'four.'\n",
    "\n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '1' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'crore':\n",
    "            CwordList[idx] = 'one crore'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "\n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '1' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'crore.':\n",
    "            CwordList[idx] = 'one crore.'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '60':\n",
    "            CwordList[idx] = 'sixty'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '60.':\n",
    "            CwordList[idx] = 'sixty.'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'liberalization':\n",
    "            CwordList[idx] = 'liberalisation'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'liberalization.':\n",
    "            CwordList[idx] = 'liberalisation.'\n",
    "\n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'day' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'to' and (idx +2) < len(CwordList) and CwordList[idx + 2] == 'day':\n",
    "            CwordList[idx] = 'day-to-day'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "            indexes_to_remove.append(idx + 2)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "\n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'day' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'to' and (idx +2) < len(CwordList) and CwordList[idx + 2] == 'day.':\n",
    "            CwordList[idx] = 'day-to-day.'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "            indexes_to_remove.append(idx + 2)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '3':\n",
    "            CwordList[idx] = 'three'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '3.':\n",
    "            CwordList[idx] = 'three.'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'globalization':\n",
    "            CwordList[idx] = 'globalisation'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'globalization.':\n",
    "            CwordList[idx] = 'globalisation.'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '2':\n",
    "            CwordList[idx] = 'two'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '2.':\n",
    "            CwordList[idx] = 'two.'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '6th':\n",
    "            CwordList[idx] = 'sixth'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '6th.':\n",
    "            CwordList[idx] = 'sixth.'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '9th':\n",
    "            CwordList[idx] = 'ninth'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '9th.':\n",
    "            CwordList[idx] = 'ninth.'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'G.S.T.':\n",
    "            CwordList[idx] = 'GST'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'demonetisation':\n",
    "            CwordList[idx] = 'demonetization'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'demonetisation.':\n",
    "            CwordList[idx] = 'demonetization.'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'Kharif':\n",
    "            CwordList[idx] = 'kharif'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'Kharif.':\n",
    "            CwordList[idx] = 'kharif.'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'Rabi':\n",
    "            CwordList[idx] = 'rabi'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'Rabi.':\n",
    "            CwordList[idx] = 'rabi.'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'Seasons':\n",
    "            CwordList[idx] = 'seasons'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'Seasons.':\n",
    "            CwordList[idx] = 'seasons.'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'Monsoon':\n",
    "            CwordList[idx] = 'monsoon'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'Monsoon.':\n",
    "            CwordList[idx] = 'monsoon.'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '5':\n",
    "            CwordList[idx] = 'five'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '5.':\n",
    "            CwordList[idx] = 'five.'\n",
    "\n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'defecation' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'free':\n",
    "            CwordList[idx] = 'Defecation Free'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "\n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'defecation' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'free.':\n",
    "            CwordList[idx] = 'Defecation Free.'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '4':\n",
    "            CwordList[idx] = 'four'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '4.':\n",
    "            CwordList[idx] = 'four.'\n",
    "\n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'credit' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'based':\n",
    "            CwordList[idx] = 'credit-based'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "\n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'credit' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'based.':\n",
    "            CwordList[idx] = 'credit-based.'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '5000':\n",
    "            CwordList[idx] = 'five thousand'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '5000.':\n",
    "            CwordList[idx] = 'five thousand.'\n",
    "\n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '5' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'thousand':\n",
    "            CwordList[idx] = 'five thousand'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "\n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '5' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'thousand.':\n",
    "            CwordList[idx] = 'five thousand.'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'scheduled':\n",
    "            CwordList[idx] = 'Scheduled'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'scheduled.':\n",
    "            CwordList[idx] = 'Scheduled.'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'castes':\n",
    "            CwordList[idx] = 'Castes'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'castes.':\n",
    "            CwordList[idx] = 'Castes.'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'tribes':\n",
    "            CwordList[idx] = 'Tribes'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'tribes.':\n",
    "            CwordList[idx] = 'Tribes.'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'agro-processing':\n",
    "            CwordList[idx] = 'agro processing'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'agro-processing.':\n",
    "            CwordList[idx] = 'agro processing.'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'post-graduate':\n",
    "            CwordList[idx] = 'post graduate'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'post-graduate.':\n",
    "            CwordList[idx] = 'post graduate.'\n",
    "    \n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'multi-model':\n",
    "            CwordList[idx] = 'multi model'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'multi-model.':\n",
    "            CwordList[idx] = 'multi model.'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'budget':\n",
    "            CwordList[idx] = 'Budget'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'budget.':\n",
    "            CwordList[idx] = 'Budget.'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'budgets':\n",
    "            CwordList[idx] = 'Budgets'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'budgets.':\n",
    "            CwordList[idx] = 'Budgets.'\n",
    "\n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'Do' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'you' and (idx +2) < len(CwordList) and CwordList[idx + 2] == 'think':\n",
    "            CwordList[idx] = 'Do not think'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "            indexes_to_remove.append(idx + 2)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "\n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'Do' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'you' and (idx +2) < len(CwordList) and CwordList[idx + 2] == 'think.':\n",
    "            CwordList[idx] = 'Do not think.'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "            indexes_to_remove.append(idx + 2)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "\n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'do' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'you' and (idx +2) < len(CwordList) and CwordList[idx + 2] == 'think':\n",
    "            CwordList[idx] = 'do not think'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "            indexes_to_remove.append(idx + 2)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "\n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'do' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'you' and (idx +2) < len(CwordList) and CwordList[idx + 2] == 'think.':\n",
    "            CwordList[idx] = 'do not think.'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "            indexes_to_remove.append(idx + 2)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "\n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'are' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'lot' and (idx +2) < len(CwordList) and CwordList[idx + 2] == 'of':\n",
    "            CwordList[idx] = 'are lots of'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "            indexes_to_remove.append(idx + 2)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "\n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'are' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'lot' and (idx +2) < len(CwordList) and CwordList[idx + 2] == 'of.':\n",
    "            CwordList[idx] = 'are lots of.'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "            indexes_to_remove.append(idx + 2)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "\n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'economic' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'survey':\n",
    "            CwordList[idx] = 'Economic Survey'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "\n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'economic' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'survey.':\n",
    "            CwordList[idx] = 'Economic Survey.'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'agro-based':\n",
    "            CwordList[idx] = 'agro based'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'agro-based.':\n",
    "            CwordList[idx] = 'agro based.'\n",
    "\n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'job' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'oriented':\n",
    "            CwordList[idx] = 'job-oriented'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "\n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'job' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'oriented.':\n",
    "            CwordList[idx] = 'job-oriented.'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'short-term':\n",
    "            CwordList[idx] = 'short term'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'short-term.':\n",
    "            CwordList[idx] = 'short term.'\n",
    "    \n",
    "    Ctext = ' '.join(CwordList)\n",
    "    Ctext = Ctext.strip()\n",
    "\n",
    "    return Ctext\n",
    "\n",
    "def main(main_pth):\n",
    "    start_time = time.time()\n",
    "    cols = ['asr_rollno', 'asr_region_code', 'asr_date_appeared', 'asr_batch_no', 'asr_spelling_mistakes', 'asr_half_error', 'asr_misc_error', 'asr_punctuation_error','asr_total_mistakes', 'asr_no_of_word_original', 'asr_per_of_error', 'asr_stenograde', 'asr_remarks']\n",
    "    ldc_error = pd.DataFrame(columns= cols)\n",
    "\n",
    "    main_folder = os.listdir(main_pth)\n",
    "    file1_pth = \"\"\n",
    "    file2_pth = \"\"\n",
    "    allowed_spellings_pth = \"\"\n",
    "    asr_remarks = \"Steno-English\"\n",
    "    asr_stenograde = \"Grade-D\"\n",
    "    temp_root = r\"C:\\Users\\AnshChoudhary\\Downloads\\Skilltest-2024\\Skilltest-2024\\Evaluated\\4th Jan- S1\\temp_text\"\n",
    "\n",
    "    for root, dirs, files in os.walk(main_pth):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                allowed_spellings_pth = os.path.join(root, file)  \n",
    "                #print(allowed_spellings_pth)  \n",
    "            elif file.endswith(\".txt\"):\n",
    "                if file.lower().startswith(\"master\"):\n",
    "                    ofile2_pth = os.path.join(root, file)\n",
    "\n",
    "                    temp_file2 = candidate_replacement(ofile2_pth)\n",
    "                    temp_pth = os.path.join(temp_root,\"master.txt\") \n",
    "                    with open(temp_pth, 'w') as file2:\n",
    "                        file2.write(temp_file2)\n",
    "                    \n",
    "                    file2_pth = temp_pth\n",
    "\n",
    "                else:\n",
    "                    ofile1_pth = os.path.join(root, file)\n",
    "                    \n",
    "                    path_lst = ofile1_pth.split('\\\\')\n",
    "                    f = path_lst[-1].split('.')\n",
    "                    asr_rollno = f[-2]\n",
    "                    \n",
    "                    first_digit = str(asr_rollno)[0]\n",
    "                    if first_digit == '1':\n",
    "                        asr_region_code = 'NWR'\n",
    "                    elif first_digit == '2':\n",
    "                        asr_region_code = 'NR'\n",
    "                    elif first_digit == '3':\n",
    "                        asr_region_code = 'CR'\n",
    "                    elif first_digit == '4':\n",
    "                        asr_region_code = 'ER'\n",
    "                    elif first_digit == '5':\n",
    "                        asr_region_code = 'NCR'\n",
    "                    elif first_digit == '6':\n",
    "                        asr_region_code = 'MPR'\n",
    "                    elif first_digit == '7':\n",
    "                        asr_region_code = 'WR'\n",
    "                    elif first_digit == '8':\n",
    "                        asr_region_code = 'SR'\n",
    "                    elif first_digit == '9':\n",
    "                        asr_region_code = 'KKR'\n",
    "                        \n",
    "                    asr_date_appeared = '04-01-2024'\n",
    "                    asr_batch_no =  'Shift-2'\n",
    "\n",
    "                    temp_file1 = candidate_replacement(ofile1_pth)\n",
    "                    temp_pth = os.path.join(temp_root,f\"{asr_rollno}.txt\") \n",
    "                    with open(temp_pth, 'w') as file1:\n",
    "                        file1.write(temp_file1)\n",
    "\n",
    "                    file1_pth = temp_pth\n",
    "\n",
    "\n",
    "                    total_spelling_mistakes, H_ERROR_CNT, F_ERROR_CNT, total_mistakes, total_words, mistake_percentage, result8 = calculate_mistake_percentage(file1_pth, file2_pth, ofile1_pth, ofile2_pth) \n",
    "                    \n",
    "                    row = {'asr_rollno': asr_rollno,\n",
    "                           'asr_region_code': asr_region_code,\n",
    "                           'asr_date_appeared': asr_date_appeared,\n",
    "                           'asr_batch_no': asr_batch_no,\n",
    "                           'asr_remarks': asr_remarks,\n",
    "                           'asr_stenograde': asr_stenograde,\n",
    "                           'asr_spelling_mistakes': total_spelling_mistakes,\n",
    "                           'asr_half_error': H_ERROR_CNT,\n",
    "                           'asr_misc_error': F_ERROR_CNT,\n",
    "                           'asr_punctuation_error': result8,\n",
    "                           'asr_total_mistakes': total_mistakes,\n",
    "                           'asr_no_of_word_original': 800,\n",
    "                           'asr_per_of_error': mistake_percentage}\n",
    "\n",
    "                    ldc_error = pd.concat([ldc_error, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "    output_excel_path = r\"C:\\Users\\AnshChoudhary\\Downloads\\Skilltest-2024\\Skilltest-2024\\Evaluation\\4th Jan- S1\\Percentage_report 4th Jan- S1.xlsx\"\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    print(total_time)\n",
    "    ldc_error.to_excel(output_excel_path, index=False)\n",
    "\n",
    "main(r\"C:\\Users\\AnshChoudhary\\Downloads\\Skilltest-2024\\Skilltest-2024\\Evaluation\\4th Jan- S1\") \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2617f16e-396e-4bca-a15f-12f2b070a24d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54d25424-cec9-41d3-9a47-2da71c5c9a85",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4eda2e0-29ac-4c85-8a63-6d2cf0f14035",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7619a157-4f99-45cb-b3f0-7c6e2952432a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
