{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ac0d113-5635-4e32-ab86-c25f80fc11f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AnshChoudhary\\AppData\\Local\\Temp\\ipykernel_12296\\1915974265.py:1349: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  ldc_error = pd.concat([ldc_error, pd.DataFrame([row])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5234.635542392731\n"
     ]
    }
   ],
   "source": [
    "import difflib # Used for sequence matching\n",
    "import string # Used for String Operations\n",
    "import re # Used for Regex Operations\n",
    "from difflib import unified_diff\n",
    "from fuzzywuzzy import fuzz\n",
    "import nltk\n",
    "import os \n",
    "import pandas as pd # Used for working on Dataframes\n",
    "import time\n",
    "import os \n",
    "import pandas as pd # Used for working on Dataframes\n",
    "from openpyxl.workbook import Workbook\n",
    "import pdfkit\n",
    "    \n",
    "def get_word_context(words, index, context_size):\n",
    "    start = max(0, index - context_size)\n",
    "    end = min(len(words), index + context_size + 1)\n",
    "    context = ' '.join(words[start:end])\n",
    "    return context.strip()\n",
    "\n",
    "def get_aheadword_context(words, index, context_size):\n",
    "    start = min(len(words), index + 1)\n",
    "    end = min(len(words), index + context_size + 1)\n",
    "    context = ' '.join(words[start:end])\n",
    "    return context.strip()\n",
    "\n",
    "def get_behindword_context(words, index, context_size):\n",
    "    start = max(0, index - context_size)\n",
    "    end = index  # Fix: Set end to index\n",
    "    context = ' '.join(words[start:end])\n",
    "    return context.strip()\n",
    "\n",
    "def remove_commasandfullstop(text):\n",
    "    # Remove commas from the text\n",
    "    cleaned_text = text.replace(',', '').replace('ред', '')\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "def singularize_with_spacy(word):\n",
    "    doc = nlp(word)\n",
    "    singularized_words = [token.lemma_ for token in doc]\n",
    "    return singularized_words[0] if singularized_words else word\n",
    "\n",
    "def word_by_word_diff(a, b):\n",
    "    len_a = len(a)\n",
    "    len_b = len(b)\n",
    "    def get_word_context(words, index, context_size):\n",
    "            start = max(0, index - context_size)\n",
    "            end = min(len(words), index + context_size + 1)\n",
    "            context = ' '.join(words[start:end])\n",
    "            return context\n",
    "\n",
    "    # Initialize a matrix to track matches between words\n",
    "    matrix = [[0] * (len_b + 1) for _ in range(len_a + 1)]\n",
    "\n",
    "    # Fill the matrix to identify matching words\n",
    "    for i in range(len_a + 1):\n",
    "        for j in range(len_b + 1):\n",
    "            if i == 0 or j == 0:\n",
    "                matrix[i][j] = 0\n",
    "            elif a[i - 1] == b[j - 1]:\n",
    "                matrix[i][j] = matrix[i - 1][j - 1] + 1\n",
    "            else:\n",
    "                matrix[i][j] = max(matrix[i - 1][j], matrix[i][j - 1])\n",
    "\n",
    "    # Trace the matrix to identify differences\n",
    "    i, j = len_a, len_b\n",
    "    operations = []\n",
    "    while i > 0 or j > 0:\n",
    "        if i > 0 and j > 0 and a[i - 1] == b[j - 1]:\n",
    "            cntxt_similarity = fuzz.ratio(get_word_context(a, i-1, 1), get_word_context(b, j-1, 1))\n",
    "            if cntxt_similarity > 70:\n",
    "                operations.append(('equal', i - 1, j - 1))\n",
    "                i -= 1\n",
    "                j -= 1\n",
    "            else:\n",
    "                operations.append(('insert', j - 1))\n",
    "                j -= 1\n",
    "                operations.append(('delete', i - 1))\n",
    "                i -= 1\n",
    "        else:\n",
    "            if j > 0 and (i == 0 or matrix[i][j - 1] >= matrix[i - 1][j]):\n",
    "                operations.append(('insert', j - 1))\n",
    "                j -= 1\n",
    "            elif i > 0 and (j == 0 or matrix[i][j - 1] < matrix[i - 1][j]):\n",
    "                operations.append(('delete', i - 1))\n",
    "                i -= 1\n",
    "\n",
    "    # Reverse the operations to maintain order\n",
    "    operations.reverse()\n",
    "\n",
    "    # Generate the word list with tags\n",
    "    output = []\n",
    "    for op in operations:\n",
    "        if op[0] == 'equal':\n",
    "            output.append((f' {a[op[1]]}'))\n",
    "        elif op[0] == 'insert':\n",
    "            output.append((f'+{b[op[1]]}'))\n",
    "        elif op[0] == 'delete':\n",
    "            output.append((f'-{a[op[1]]}'))\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# Function to evaluate mistakes\n",
    "def Numberof_mistakes(Mtext, Ctext):\n",
    "\n",
    "    c = 0  # Count for substituted words\n",
    "    y = 0  # Count for misspelled words\n",
    "\n",
    "    # Split candidate text into sentences\n",
    "    sentences = nltk.sent_tokenize(Ctext)\n",
    "    Titlerror_List=[]  # List to hold words with title case errors\n",
    "\n",
    "    Mtext = remove_commasandfullstop(Mtext)\n",
    "    Ctext = remove_commasandfullstop(Ctext)\n",
    "    \n",
    "    MwordList = Mtext.split()\n",
    "    MwordList.pop(0)\n",
    "    \n",
    "    CwordList = Ctext.split()\n",
    "    CwordList = [word.replace('\\u200d', '') for word in CwordList]\n",
    "    \n",
    "    master_Wtotal = len(MwordList)\n",
    "    candidate_Wtotal = len(CwordList)\n",
    "    \n",
    "    differ = list(unified_diff(MwordList, CwordList))\n",
    "    segment_lenM = []\n",
    "    segment_lenC = []\n",
    "\n",
    "    for line in differ:\n",
    "        if line.startswith('@'):\n",
    "            a_match = re.search(r'-(\\d+),', line)\n",
    "            b_match = re.search(r'\\+(\\d+),', line)\n",
    "            \n",
    "            a = int(a_match.group().split(',')[0]) if a_match else 0\n",
    "            b = int(b_match.group().split(',')[0]) if b_match else 0\n",
    "\n",
    "            segment_lenM.append(abs(a))\n",
    "            segment_lenC.append(b)\n",
    "\n",
    "    M_chunks = []\n",
    "    start = 0\n",
    "\n",
    "    for length in segment_lenM:\n",
    "        M_chunks.append(MwordList[start:length])\n",
    "        start = length\n",
    "\n",
    "    M_chunks.append(MwordList[start:]) # Adding the remaining elements to the last list\n",
    "\n",
    "    C_chunks = []\n",
    "    start = 0\n",
    "\n",
    "    for length in segment_lenC:\n",
    "        C_chunks.append(CwordList[start:length])\n",
    "        start = length\n",
    "\n",
    "    C_chunks.append(CwordList[start:]) # Adding the remaining elements to the last list\n",
    "\n",
    "    differ = []\n",
    "\n",
    "    inx = 0\n",
    "    # Compare each chunk separately\n",
    "    while inx < len(M_chunks):\n",
    "        diff = list(word_by_word_diff(C_chunks[inx], M_chunks[inx]))\n",
    "        if diff:\n",
    "            for line in diff:\n",
    "                differ.append(line)\n",
    "        inx+=1\n",
    "\n",
    "    M_index = 0\n",
    "    C_index = 0\n",
    "    t = 0\n",
    "    tupple=[]\n",
    "    omitted=[]\n",
    "    added=[]\n",
    "    l = 0\n",
    "\n",
    "    # similarity = fuzz.ratio('ansh.', 'ansh,')\n",
    "    # print(similarity)\n",
    "\n",
    "    mastertext = [element for element in differ if not element.startswith('+')]\n",
    "    candidatetext = [element for element in differ if not element.startswith('-')]\n",
    "    \n",
    "    for i, item in enumerate(differ):\n",
    "        if i >= l:\n",
    "            if item.startswith('-'):  # Checks if the item in the 'differ' list represents a deletion in the master text\n",
    "                j = i\n",
    "                check1 = []\n",
    "                check2 = []\n",
    "                while j < len(differ) and not differ[j].startswith(' '):\n",
    "                    if differ[j].startswith('-'):\n",
    "                        word1 = differ[j]  # Extracts the word that is deleted in the master text\n",
    "                        # Locate word1 in the master text\n",
    "                        while M_index < len(MwordList) and not mastertext[M_index] == word1:\n",
    "                            M_index += 1  # Moves through the master text word list until it finds 'word1'\n",
    "\n",
    "                        word1 = word1[1:]\n",
    "                        check1.append((word1, M_index))\n",
    "                        M_index += 1\n",
    "                    if differ[j].startswith('+'):\n",
    "                        word2 = differ[j] # Extracts the word that is added in the candidate text\n",
    "\n",
    "                        # Locate word2 in the candidate text\n",
    "                        while C_index < len(CwordList) and not candidatetext[C_index] == word2:\n",
    "                            C_index += 1  # Moves through the candidate text word list until it finds 'word2'\n",
    "\n",
    "                        word2 = word2[1:]\n",
    "                        check2.append((word2, C_index))\n",
    "                        C_index += 1\n",
    "                    j += 1\n",
    "                    l = j\n",
    "                m = 0\n",
    "                # print(check2)\n",
    "                # print(check1)\n",
    "\n",
    "                all_matched_words = []\n",
    "                replaced_words = []\n",
    "                for y, tuple2 in enumerate(check2):\n",
    "                    element2, index2 = tuple2\n",
    "                    empty = []\n",
    "                    for x, tuple1 in enumerate(check1):\n",
    "                        element1, index1 = tuple1\n",
    "                        if x>=m:\n",
    "                            similarity = fuzz.ratio(element2, element1)\n",
    "                            ahead_similarity = fuzz.ratio(get_aheadword_context(MwordList, index2, 10), get_aheadword_context(CwordList, index1, 10))\n",
    "                            behind_similarity = fuzz.ratio(get_behindword_context(MwordList, index2, 10), get_behindword_context(CwordList, index1, 10))\n",
    "                            if get_word_context(MwordList, index2, 1) and get_word_context(CwordList, index1, 1):\n",
    "                                cntxt_similarity = fuzz.ratio(get_word_context(MwordList, index2, 1), get_word_context(CwordList, index1, 1))\n",
    "                                if similarity > 60 and cntxt_similarity>60:\n",
    "                                    empty.append((((element2, index2), (element1, index1)), similarity, x))\n",
    "                                elif similarity == 100 and (ahead_similarity>70 or behind_similarity>70):\n",
    "                                    empty.append((((element2, index2), (element1, index1)), similarity, x))\n",
    "                                elif similarity > 65 and (ahead_similarity>70 or behind_similarity>70):\n",
    "                                    empty.append((((element2, index2), (element1, index1)), similarity, x))\n",
    "\n",
    "                    sorted_empty = sorted(empty, key=lambda x: x[1], reverse=True)  # Sort based on the second element of each tuple\n",
    "                    if len(sorted_empty)>0:\n",
    "                        # tupple.append(sorted_empty[0][0])\n",
    "                        all_matched_words.append((sorted_empty[0][0], y, sorted_empty[0][2]))\n",
    "                        replaced_words.append((y, sorted_empty[0][2]))\n",
    "                        m = sorted_empty[0][2] + 1\n",
    "\n",
    "                for i, x in enumerate(all_matched_words):\n",
    "                    if (i+1) in range(len(all_matched_words)):\n",
    "                        midx1 = x[1]\n",
    "                        cidx1 = x[2]\n",
    "                        midx2 = all_matched_words[i+1][1]\n",
    "                        cidx2 = all_matched_words[i+1][2]\n",
    "\n",
    "                        if (midx2 - midx1) > 0 and (cidx2 - cidx1) > 0:\n",
    "                            mrange = [m for m in range((midx1 + 1), midx2)]\n",
    "                            crange = [n for n in range((cidx1 + 1), cidx2)]\n",
    "\n",
    "                            # Pair elements from mrange and crange\n",
    "                            paired_elements = list(zip(mrange, crange))\n",
    "                            for pair in paired_elements:\n",
    "                                if len(pair)>0:\n",
    "                                    replaced_words.append(pair)\n",
    "                    else:\n",
    "                        midx1 = x[1]\n",
    "                        cidx1 = x[2]\n",
    "\n",
    "                        if (len(check2) -1 -midx1) > 0 and (len(check1) -1 - cidx1) > 0:\n",
    "                            mrange = [m for m in range((midx1 + 1), (len(check2)))]\n",
    "                            crange = [n for n in range((cidx1 + 1), (len(check1)))]\n",
    "\n",
    "                            # Pair elements from mrange and crange\n",
    "                            paired_elements = list(zip(mrange, crange))\n",
    "                            for pair in paired_elements:\n",
    "                                if len(pair)>0:\n",
    "                                    replaced_words.append(pair)\n",
    "                if len(all_matched_words)>0:\n",
    "                    if all_matched_words[0][1]>0 and all_matched_words[0][2]>0:\n",
    "                        midx2 = all_matched_words[0][1]\n",
    "                        cidx2 = all_matched_words[0][2]\n",
    "\n",
    "                        if (midx2) > 0 and (cidx2) > 0:\n",
    "                            mrange = [m for m in range(0, midx2)]\n",
    "                            crange = [n for n in range(0, cidx2)]\n",
    "                            \n",
    "                            # Pair elements from mrange and crange\n",
    "                            paired_elements = list(zip(mrange, crange))\n",
    "                            for pair in paired_elements:\n",
    "                                if len(pair)>0:\n",
    "                                    replaced_words.append(pair)\n",
    "\n",
    "                if len(all_matched_words)==0:\n",
    "                    mrange = [m for m in range(0, len(check2))]\n",
    "                    crange = [n for n in range(0, len(check1))]\n",
    "                    # Pair elements from mrange and crange\n",
    "                    paired_elements = list(zip(mrange, crange))\n",
    "                    for pair in paired_elements:\n",
    "                        if len(pair)>0:\n",
    "                            replaced_words.append(pair)\n",
    "\n",
    "                replaced_words = sorted(replaced_words, key=lambda x:x[0])\n",
    "                for one in replaced_words:\n",
    "                    a1, a2 = one\n",
    "                    tupple.append((check2[a1], check1[a2]))\n",
    "\n",
    "                empty = []\n",
    "                if len(check2)>0:\n",
    "                    for om_i, om_tup in enumerate(check2):\n",
    "                        for tu in replaced_words:\n",
    "                            mi = tu[0]\n",
    "                            empty.append(mi)\n",
    "\n",
    "                        if om_i not in empty:\n",
    "                            omitted.append(om_tup)\n",
    "\n",
    "                empty = []\n",
    "                if len(check1)>0:\n",
    "                    for ad_i, ad_tup in enumerate(check1):\n",
    "                        for tu in replaced_words:\n",
    "                            ci = tu[1]\n",
    "                            empty.append(ci)\n",
    "\n",
    "                        if ad_i not in empty:\n",
    "                            added.append(ad_tup)\n",
    "\n",
    "            elif item.startswith(' '):\n",
    "                M_index += 1\n",
    "                C_index += 1\n",
    "            elif item.startswith('+'):\n",
    "                omitted.append((item[1:], C_index))\n",
    "                C_index += 1\n",
    "                \n",
    "    a = len(omitted)  # Omitted words count\n",
    "    b = len(added)  # Added words count\n",
    "    y = 0\n",
    "    Mispelled_List = []  # List to hold misspelled words\n",
    "    Substitute = []  # List to hold substituted \n",
    "    for inx, tup in enumerate(tupple):\n",
    "        Mw, Cw = tup\n",
    "        if Mw[0] != Cw[0]:\n",
    "            similarity = fuzz.ratio(Mw[0], Cw[0])\n",
    "            if similarity > 65 :#and (min((len(Mw[0])), len(Cw[0]))/max(len(Mw[0]), len(Cw[0])))> 0.5:  # Check if the candidate word is a misspelling of the master word\n",
    "                Mispelled_List.append(tup)  # Collect misspelled words\n",
    "                y+=1\n",
    "            else:\n",
    "                c += 1\n",
    "                Substitute.append(tup)  # Collect wrongly substituted words\n",
    "\n",
    "    for i, tup in enumerate(Mispelled_List):\n",
    "        p, q = tup\n",
    "        p1, p2 = p\n",
    "        q1, q2 = q\n",
    "        Mispelled_List[i] = (p1, q1, p2, q2)\n",
    "        \n",
    "    intex_to_delete = []\n",
    "    for i, tup in enumerate(Substitute):\n",
    "        p, q = tup\n",
    "        p1, p2 = p\n",
    "        q1, q2 = q\n",
    "        if p1 == '11' and q1 == 'рдЧреНрдпрд╛рд░рд╣':\n",
    "            intex_to_delete.append(i)\n",
    "        if p1 == '10' and q1 == 'рджрд╕':\n",
    "            intex_to_delete.append(i)\n",
    "        if p1== 'рдУрд░' and q1 == 'рдФрд░' and p2 == 726:\n",
    "            intex_to_delete.append(i)\n",
    "        Substitute[i] = (p1, q1, p2, q2)\n",
    "        \n",
    "    intex_to_delete.sort(reverse=True)\n",
    "    for idx in intex_to_delete:\n",
    "        Substitute.pop(idx)\n",
    "\n",
    "    return a, b, c, y, omitted, added, Mispelled_List, Substitute\n",
    "\n",
    "def compare_punctuation_latest(file1_path, file2_path):\n",
    "\n",
    "    def has_punctuation(word):\n",
    "        return any(char in 'ред' for char in word)\n",
    "\n",
    "    def is_char_punctuation(character):\n",
    "        if character in 'ред':\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def get_word_context(words, index, context_size):\n",
    "        start = max(0, index - context_size)\n",
    "        end = min(len(words), index + context_size + 1)\n",
    "        context = ' '.join(words[start:end])\n",
    "        return context\n",
    "\n",
    "    with open(file1_path, 'r', encoding = 'utf-8') as file1:\n",
    "        candidate_txt = file1.read()\n",
    "\n",
    "    with open(file2_path, 'r', encoding = 'utf-8') as file2:\n",
    "        master_txt = file2.read()\n",
    "\n",
    "    candidate_words = candidate_txt.split()\n",
    "    master_words = master_txt.split()\n",
    "    results = []\n",
    "    words_with_punct_master = []\n",
    "    words_with_punct_candidate = []\n",
    "\n",
    "    for mindx, mstr_wrd in enumerate(master_words):\n",
    "        if has_punctuation(mstr_wrd):\n",
    "            words_with_punct_master.append((mstr_wrd, mindx))\n",
    "\n",
    "    #print(\"_________________________MASTER WORDS_____________________\")\n",
    "    for cindx, cand_wrd in enumerate(candidate_words):\n",
    "        if has_punctuation(cand_wrd):\n",
    "            words_with_punct_candidate.append((cand_wrd, cindx))\n",
    "\n",
    "    for mtup in words_with_punct_master:\n",
    "        m_context = get_word_context(master_words, mtup[1], 2)\n",
    "        best_match_context = ''\n",
    "        match_rtio = 0\n",
    "        cand_word_detail = ''\n",
    "        for cnd_idx, cnd_wrd in enumerate(candidate_words):\n",
    "            c_context = get_word_context(candidate_words, cnd_idx, 2)\n",
    "            ratio = fuzz.ratio(m_context.lower(), c_context.lower())\n",
    "            if ratio>match_rtio:\n",
    "                best_match_context = c_context\n",
    "                match_rtio = ratio\n",
    "                cand_word_detail = (mtup[0], mtup[1],  cnd_wrd, cnd_idx)\n",
    "       # print(m_context, \"==||==\", best_match_context, \"==|==\",mtup, \"==|==\",match_rtio)\n",
    "        for charactr in mtup[0]:\n",
    "            if is_char_punctuation(charactr):\n",
    "                if charactr not in best_match_context and match_rtio>70:\n",
    "                    if mtup not in results:    \n",
    "                        punct_mark = ''.join(c for c in mtup[0] if not c.isalnum())\n",
    "                        results.append(f\"{{{mtup[0]}, {mtup[1]}, ({punct_mark}), Punctuation Missed}};\")\n",
    "                elif mtup[0] not in best_match_context:\n",
    "                    match_found = 0\n",
    "                    for wrd in best_match_context.split():\n",
    "                        if fuzz.ratio(mtup[0].lower(), wrd.lower())>70:\n",
    "                            match_found = 1\n",
    "                            break\n",
    "                    if match_found==0:\n",
    "                        if mtup not in results:\n",
    "                            punct_mark = ''.join(c for c in mtup[0] if not c.isalnum())\n",
    "                            results.append(f\"{{{mtup[0]}, {mtup[1]}, ({punct_mark}), Punctuation Missed}};\")\n",
    "\n",
    "\n",
    "    for ctup in words_with_punct_candidate:\n",
    "        cand_context = get_word_context(candidate_words, ctup[1], 2)\n",
    "        best_mtch_context = ''\n",
    "        mtch_rtio = 0\n",
    "        mstr_word_detail = ''\n",
    "        for mstr_idx, master_wrd in enumerate(master_words):\n",
    "            mster_context = get_word_context(master_words, mstr_idx, 2)\n",
    "            mtch_ratio = fuzz.ratio(mster_context.lower(), cand_context.lower())\n",
    "            if mtch_ratio>mtch_rtio:\n",
    "                best_mtch_context = mster_context\n",
    "                mtch_rtio = mtch_ratio\n",
    "                mstr_word_detail = (ctup[0], ctup[1],  master_wrd, mstr_idx)\n",
    "        for charctr in ctup[0]:\n",
    "            if is_char_punctuation(charctr):\n",
    "                if charctr not in best_mtch_context:\n",
    "                    if ctup not in results:\n",
    "                        punct_mark = ''.join(c for c in ctup[0] if not c.isalnum())\n",
    "                        results.append(f\"{{{ctup[0]}, {ctup[1]}, ({punct_mark}), Punctuation Added}};\")\n",
    "                        \n",
    "    return results\n",
    "\n",
    "def Splconandtrans(file2_path, file1_path):\n",
    "    with open(file2_path, 'r', encoding='utf-8') as file2:\n",
    "        Mtext = file2.read()\n",
    "\n",
    "    with open(file1_path, 'r', encoding='utf-8') as file1:\n",
    "        Ctext = file1.read()\n",
    "\n",
    "    c = 0  # Count for substituted words\n",
    "    y = 0  # Count for misspelled words\n",
    "\n",
    "    Mtext = remove_commasandfullstop(Mtext)\n",
    "    Ctext = remove_commasandfullstop(Ctext)\n",
    "\n",
    "    MwordList = Mtext.split()\n",
    "    MwordList.pop(0)\n",
    "    \n",
    "    CwordList = Ctext.split()\n",
    "    CwordList = [word.replace('\\u200d', '') for word in CwordList]\n",
    "\n",
    "    master_Wtotal = len(MwordList)\n",
    "    candidate_Wtotal = len(CwordList)\n",
    "\n",
    "    differ = list(unified_diff(MwordList, CwordList))\n",
    "    segment_lenM = []\n",
    "    segment_lenC = []\n",
    "\n",
    "    for line in differ:\n",
    "        if line.startswith('@'):\n",
    "            a_match = re.search(r'-(\\d+),', line)\n",
    "            b_match = re.search(r'\\+(\\d+),', line)\n",
    "            \n",
    "            a = int(a_match.group().split(',')[0]) if a_match else 0\n",
    "            b = int(b_match.group().split(',')[0]) if b_match else 0\n",
    "\n",
    "            segment_lenM.append(abs(a))\n",
    "            segment_lenC.append(b)\n",
    "\n",
    "    M_chunks = []\n",
    "    start = 0\n",
    "\n",
    "    for length in segment_lenM:\n",
    "        M_chunks.append(MwordList[start:length])\n",
    "        start = length\n",
    "\n",
    "    M_chunks.append(MwordList[start:]) # Adding the remaining elements to the last list\n",
    "\n",
    "    C_chunks = []\n",
    "    start = 0\n",
    "\n",
    "    for length in segment_lenC:\n",
    "        C_chunks.append(CwordList[start:length])\n",
    "        start = length\n",
    "\n",
    "    C_chunks.append(CwordList[start:]) # Adding the remaining elements to the last list\n",
    "\n",
    "    differ = []\n",
    "\n",
    "    inx = 0\n",
    "    # Compare each chunk separately\n",
    "    while inx < len(M_chunks):\n",
    "        diff = list(word_by_word_diff(C_chunks[inx], M_chunks[inx]))\n",
    "        if diff:\n",
    "            for line in diff:\n",
    "                differ.append(line)\n",
    "        inx+=1\n",
    "\n",
    "    M_index = 0\n",
    "    C_index = 0\n",
    "    t = 0\n",
    "    tupple=[]\n",
    "    omitted=[]\n",
    "    added=[]\n",
    "    l = 0\n",
    "\n",
    "    # similarity = fuzz.ratio('ansh.', 'ansh,')\n",
    "    # print(similarity)\n",
    "\n",
    "    mastertext = [element for element in differ if not element.startswith('+')]\n",
    "    candidatetext = [element for element in differ if not element.startswith('-')]\n",
    "    \n",
    "    splits = []\n",
    "    concat = []\n",
    "    transposed = []\n",
    "    for i, item in enumerate(differ):\n",
    "        if i >= l:\n",
    "            if item.startswith('-'):  # Checks if the item in the 'differ' list represents a deletion in the master text\n",
    "                j = i\n",
    "                check1 = []\n",
    "                check2 = []\n",
    "                while j < len(differ) and not differ[j].startswith(' '):\n",
    "                    if differ[j].startswith('-'):\n",
    "                        word1 = differ[j]  # Extracts the word that is deleted in the master text\n",
    "                        # Locate word1 in the master text\n",
    "                        while M_index < len(MwordList) and not mastertext[M_index] == word1:\n",
    "                            M_index += 1  # Moves through the master text word list until it finds 'word1'\n",
    "\n",
    "                        word1 = word1[1:]\n",
    "                        check1.append((word1, M_index))\n",
    "                        M_index += 1\n",
    "                    if differ[j].startswith('+'):\n",
    "                        word2 = differ[j] # Extracts the word that is added in the candidate text\n",
    "\n",
    "                        # Locate word2 in the candidate text\n",
    "                        while C_index < len(CwordList) and not candidatetext[C_index] == word2:\n",
    "                            C_index += 1  # Moves through the candidate text word list until it finds 'word2'\n",
    "\n",
    "                        word2 = word2[1:]\n",
    "                        check2.append((word2, C_index))\n",
    "                        C_index += 1\n",
    "                    j += 1\n",
    "                    l = j\n",
    "                # print(check2)\n",
    "                # print(check1)\n",
    "                extra = 0\n",
    "                for ind, mt in enumerate(check2):\n",
    "                     element2, index2 = mt\n",
    "                     for indi, cd in enumerate(check1):\n",
    "                        if indi >= extra:\n",
    "                            element1n1, index1n1 = cd\n",
    "                            if (indi+1)<len(check1):\n",
    "                                    element1n2, index1n2 = check1[indi+1]\n",
    "                                    if element2 == f'{element1n1}{element1n2}':\n",
    "                                        out = f'{{{element2}, {element1n1} {element1n2}, {index2}, {index1n1}, Splitted Word}}'\n",
    "                                        splits.append(out)\n",
    "                                        extra = indi+1\n",
    "                                    elif element2 == f'{element1n1}-{element1n2}':\n",
    "                                        out = f'{{{element2}, {element1n1} {element1n2}, {index2}, {index1n1}, Splitted Word}}'\n",
    "                                        splits.append(out)\n",
    "                                        extra = indi+1\n",
    "                                        \n",
    "                extra = 0\n",
    "                for ind, mt in enumerate(check1):\n",
    "                     element1, index1 = mt\n",
    "                     for indi, cd in enumerate(check2):\n",
    "                        if indi >= extra:\n",
    "                            element2n1, index2n1 = cd\n",
    "                            if (indi+1)<len(check2):\n",
    "                                    element2n2, index2n2 = check2[indi+1]\n",
    "                                    if element1 == f'{element2n1}{element2n2}':\n",
    "                                        out = f'{{{element2n1} {element2n2}, {element1}, {index2n1}, {index1}, concatnated Word}}'\n",
    "                                        concat.append(out)\n",
    "                                        extra = indi+1\n",
    "                                    elif element1 == f'{element2n1}-{element2n2}':\n",
    "                                        out = f'{{{element2n1} {element2n2}, {element1}, {index2n1}, {index1}, concatnated Word}}'\n",
    "                                        concat.append(out)\n",
    "                                        extra = indi+1\n",
    "                                        \n",
    "                extra = 0\n",
    "                for ind, mt in enumerate(check2):\n",
    "                    element2n1, index2n1 = mt\n",
    "                    if (ind+1)<len(check2):\n",
    "                        element2n2, index2n2 = check2[ind+1]\n",
    "                        for indi, cd in enumerate(check1):\n",
    "                            if indi >= extra:\n",
    "                                element1n1, index1n1 = cd\n",
    "                                if (indi+1)<len(check1):\n",
    "                                        element1n2, index1n2 = check1[indi+1]\n",
    "                                        if element2n1 == element1n2 and element2n2 == element1n1:\n",
    "                                            out = f'{{{element2n1} {element2n2}, {element1n1} {element1n2}, {index2n1}, {index1n1}, Transposed Word}}'\n",
    "                                            transposed.append(out)\n",
    "                                            extra = indi+1\n",
    "\n",
    "            elif item.startswith(' '):\n",
    "                M_index += 1\n",
    "                C_index += 1\n",
    "            elif item.startswith('+'):\n",
    "                added.append((item[1:], C_index))\n",
    "                C_index += 1\n",
    "                \n",
    "    return splits, concat, transposed\n",
    "\n",
    "def find_fullstop_errors(file1_path, file2_path):\n",
    "    errors = []\n",
    "\n",
    "    def count_fullstops(word):\n",
    "        return word.count('ред')\n",
    "\n",
    "    def count_indices(word_file):\n",
    "        with open(word_file, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        words_file = re.findall(r'[рдА-ре┐][\\u0900-\\u097F]+', content)\n",
    "        return words_file\n",
    "\n",
    "    word_file1 = count_indices(file1_path)\n",
    "    word_file2 = count_indices(file2_path)    \n",
    "\n",
    "            \n",
    "    def find_word_indices(word_file):\n",
    "        word_indices = {}\n",
    "        for i, word in enumerate(word_file):\n",
    "            if word.endswith('ред'):\n",
    "                word_lower = word.lower()\n",
    "                if word_lower not in word_indices:\n",
    "                    word_indices[word_lower] = []\n",
    "                word_indices[word_lower].append(i)\n",
    "        return word_indices\n",
    "\n",
    "    word_indices1 = find_word_indices(word_file1)\n",
    "    word_indices2 = find_word_indices(word_file2)\n",
    "    \n",
    "    all_valuesM = [value for sublist in word_indices2.values() for value in sublist]\n",
    "    all_valuesC = [value for sublist in word_indices1.values() for value in sublist]\n",
    "    all_valuesM = sorted(all_valuesM)\n",
    "    all_valuesC = sorted(all_valuesC)\n",
    "    # print(all_valuesM)\n",
    "    # print(all_valuesC)\n",
    "    master_found = []\n",
    "    candi_found = []\n",
    "    extra = 0\n",
    "    for i2, value2 in enumerate(all_valuesM):\n",
    "        for i1, value1 in enumerate(all_valuesC):\n",
    "            if i1>=extra:\n",
    "                similarityB = fuzz.ratio(get_behindword_context(word_file2, value2, 8), get_behindword_context(word_file1, value1, 8))\n",
    "                similarityA = fuzz.ratio(get_aheadword_context(word_file2, value2, 8), get_aheadword_context(word_file1, value1, 8))\n",
    "                # print(get_behindword_context(word_file2, value2, 8),value2 , get_behindword_context(word_file1, value1, 8), value1, similarityB)\n",
    "                # print(get_aheadword_context(word_file2, value2, 8), value2, get_aheadword_context(word_file1, value1, 8), value1, similarityA)\n",
    "                if similarityB > 75 or similarityA > 75:\n",
    "                    master_found.append(value2)\n",
    "                    candi_found.append(value1)\n",
    "                    extra = i1 + 1\n",
    "                    break\n",
    "    # print(master_found)\n",
    "    # print(candi_found)\n",
    "    for i in candi_found:\n",
    "        result = count_fullstops(word_file1[i])\n",
    "        if result > 1:\n",
    "            error_info = [word_file1[i], '99999', i, 'unnecessary fullstop']\n",
    "            errors.extend([error_info] * (result-1))\n",
    "        \n",
    "    indices2 = [value for value in all_valuesM if value not in master_found]\n",
    "    indices1 = [value for value in all_valuesC if value not in candi_found]\n",
    "    \n",
    "    for i in indices1:\n",
    "        result = count_fullstops(word_file1[i])\n",
    "        error_info = [word_file1[i], '99999', i, 'unnecessary fullstop']\n",
    "        if result> 1:\n",
    "            errors.extend([error_info] * (result-1))\n",
    "        else:\n",
    "            errors.extend([error_info])\n",
    "    for i in indices2:\n",
    "        errors.extend([[word_file2[i], i, '99999', 'missing fullstop']])\n",
    "        \n",
    "    idx = []    \n",
    "    for i in errors:\n",
    "        if i[3].startswith('miss'):\n",
    "            idx.append(i[1])\n",
    "        elif i[3].startswith('unn'):\n",
    "            idx.append(i[2])\n",
    "        idx = sorted(idx)\n",
    "    sorted_data = sorted(errors, key=lambda x: idx.index(x[1]) if x[3] == 'missing fullstop' else idx.index(x[2]))\n",
    "    sorted_data_str = \"{\" + \"}, {\".join(\", \".join(str(i) for i in item) for item in sorted_data) + \"}\"\n",
    "\n",
    "    return sorted_data, sorted_data_str\n",
    "\n",
    "def calculate_mistake_percentage(file1_path, file2_path, ofile1_pth, ofile2_pth):\n",
    "    def count_words(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as words_file:\n",
    "            words_text = words_file.read()\n",
    "            words_list = words_text.split()\n",
    "            \n",
    "        return len(words_list)\n",
    "\n",
    "    def roll_no(file_path):\n",
    "        file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        return file_name\n",
    "    \n",
    "    total_words = 0\n",
    "    full_mistakes = 0\n",
    "    half_mistakes = 0\n",
    "\n",
    "    result5, out1, result4 = Splconandtrans(file2_path, file1_path)\n",
    "    if out1:\n",
    "        half_mistakes += len(out1)\n",
    "    \n",
    "    if result5:\n",
    "        half_mistakes += len(result5)\n",
    "    # print(result5)\n",
    "    if result4:\n",
    "        half_mistakes += len(result4)\n",
    "\n",
    "    with open(file2_path, 'r', encoding = 'utf-8') as mfile:\n",
    "        master_text = mfile.read()\n",
    "    with open(file1_path, 'r', encoding = 'utf-8') as cfile:\n",
    "        candidate_text = cfile.read()\n",
    "\n",
    "    candidate_text = remove_commasandfullstop(candidate_text)\n",
    "    splitted_ctext = candidate_text.split()\n",
    "    words_to_replace = []\n",
    "    for x in out1:\n",
    "        xlist = x.split(', ')\n",
    "        splitted_ctext[int(xlist[-2])] = xlist[0][1:]\n",
    "        \n",
    "    for x in result4:\n",
    "        xlist = x.split(', ')\n",
    "        words = xlist[0].split()\n",
    "        splitted_ctext[int(xlist[-2])] = words[0][1:]\n",
    "        splitted_ctext[int(xlist[-2]) + 1] = words[1]\n",
    "    # print(splitted_ctext)\n",
    "    # print(missing_splits)\n",
    "    indexes_to_remove = []\n",
    "    for x in result5:\n",
    "        xlist = x.split(', ')\n",
    "        splitted_ctext[int(xlist[-2])] = xlist[0][1:]\n",
    "        indexes_to_remove.append(int(xlist[-2])+1)\n",
    "        \n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        splitted_ctext.pop(idx)\n",
    "    \n",
    "        \n",
    "    candidate_text = ' '.join(splitted_ctext)\n",
    "    candidate_text = candidate_text.strip()\n",
    "    candidate_text = candidate_text.lower()\n",
    "    master_text = remove_commasandfullstop(master_text)\n",
    "    master_text = master_text.lower()\n",
    "\n",
    "    a, b, c, z, missed_words, added_words, misspelled, replaced_words = Numberof_mistakes(master_text, candidate_text)\n",
    "\n",
    "    final_spelling_mistakes = ''\n",
    "    total_spelling_mistakes = 0 \n",
    "    for mstk in misspelled:\n",
    "        final_spelling_mistakes=final_spelling_mistakes+'{'+mstk[0]+', '+mstk[1]+', '+str(mstk[2])+', '+str(mstk[3])+', Spelling Mistake}; '\n",
    "        total_spelling_mistakes+=1\n",
    "\n",
    "    errors = []\n",
    "    total_replaced_words = 0 \n",
    "    for repl_wrds in replaced_words:\n",
    "        errors.extend([[repl_wrds[0], repl_wrds[1], repl_wrds[2], repl_wrds[3], 'Replaced Word']])\n",
    "        total_replaced_words+=1\n",
    "\n",
    "    #FINAL MISSED WORDS ARE STORED IN BELOW LIST\n",
    "    total_missed_words_count = 0\n",
    "    for msd_wrds in missed_words:\n",
    "        errors.extend([[msd_wrds[0], msd_wrds[1], '99999', 'Missed']])\n",
    "        total_missed_words_count+=1\n",
    "        \n",
    "    #Filter those extra added words which is already taken in spelling mistakes and replace words mistake\n",
    "    total_count_of_added_words = 0\n",
    "    for extr_add_wrd in added_words:\n",
    "        errors.extend([[extr_add_wrd[0], '99999', extr_add_wrd[1], 'Added']])\n",
    "        total_count_of_added_words+=1\n",
    "        \n",
    "    idx = []\n",
    "    for i in errors:\n",
    "        # print(i)\n",
    "        if len(i)==4:\n",
    "            if 'Missed' in i[3]:\n",
    "                idx.append(i[1])\n",
    "            elif 'Added' in i[3]:\n",
    "                idx.append(i[2])\n",
    "        else:\n",
    "            idx.append(i[2])\n",
    "        idx = sorted(idx)\n",
    "        \n",
    "    sorted_data_full = []\n",
    "    for error in errors:\n",
    "        if len(error)==4:\n",
    "            if error[3] == 'Missed':\n",
    "                sorted_data_full.append((error, idx.index(error[1])))\n",
    "            elif error[3] == 'Added':\n",
    "                sorted_data_full.append((error, idx.index(error[2])))\n",
    "        else:\n",
    "            sorted_data_full.append((error, idx.index(error[2])))\n",
    "    \n",
    "    sorted_data_full.sort(key=lambda x: x[1])\n",
    "    sorted_data_full = [item[0] for item in sorted_data_full]\n",
    "    sorted_data_full_err = \"{\" + \"}, {\".join(\", \".join(str(i) for i in item) for item in sorted_data_full) + \"}\"\n",
    "\n",
    "\n",
    "    if replaced_words:\n",
    "        full_mistakes +=  total_replaced_words\n",
    "    \n",
    "    if missed_words:\n",
    "        full_mistakes += total_missed_words_count\n",
    "\n",
    "    if added_words:\n",
    "        full_mistakes += total_count_of_added_words\n",
    "        \n",
    "    H_ERROR_CNT = half_mistakes\n",
    "    F_ERROR_CNT = full_mistakes\n",
    "    \n",
    "    if misspelled:\n",
    "        half_mistakes += total_spelling_mistakes\n",
    "            \n",
    "    sorted_data, result8 = find_fullstop_errors(ofile1_pth, ofile2_pth)\n",
    "    if sorted_data:\n",
    "        half_mistakes += len(sorted_data)\n",
    "        \n",
    "\n",
    "    with open(file2_path, 'r', encoding = 'utf-8') as file2:\n",
    "        words_file2 = file2.read().lower().split()\n",
    "        words_file2 = [word.strip(string.punctuation) for word in words_file2]\n",
    "        total_words = len(words_file2)\n",
    "\n",
    "    punctuation_set = set(string.punctuation)\n",
    "    punctuation_count = 0\n",
    "    tab_count = 0\n",
    "\n",
    "    with open(file2_path, 'r', encoding = 'utf-8') as file:\n",
    "        content = file.read()\n",
    "        for char in content:\n",
    "            if char in punctuation_set:\n",
    "                punctuation_count += 1\n",
    "\n",
    "        for line in file:\n",
    "            tab_count += line.count('\\t')\n",
    "                    \n",
    "\n",
    "    total_words_master = total_words + punctuation_count + tab_count\n",
    "    total_mistakes = (half_mistakes / 2) + full_mistakes \n",
    "    mistake_percentage = ((total_mistakes * 100) / 800)\n",
    "    \n",
    "    if mistake_percentage > 100:\n",
    "        mistake_percentage = 100\n",
    "\n",
    "    path_to_wkhtmltopdf = r'C:\\Program Files\\wkhtmltopdf\\bin\\wkhtmltopdf.exe'\n",
    "\n",
    "    config = pdfkit.configuration(wkhtmltopdf=path_to_wkhtmltopdf)\n",
    "\n",
    "    # Define your result variables\n",
    "    missing_space = out1\n",
    "    transposed_errors = result4\n",
    "    splitted_words = result5\n",
    "    misspelled_words = final_spelling_mistakes\n",
    "    combined_full = sorted_data_full_err\n",
    "    fullstop_err = result8\n",
    "    roll_no = roll_no(file1_path)\n",
    "    words_count1 = count_words(file1_path)\n",
    "    \n",
    "\n",
    "    # Function to format error details as a string\n",
    "    def format_error_details(error_list):\n",
    "        if len(error_list)>0:\n",
    "            return ''.join(error_list)\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "    # Categorize errors\n",
    "    spelling_mistake = format_error_details(misspelled_words)\n",
    "    other_than_spelling_mistake = format_error_details(combined_full) \n",
    "    \n",
    "    # Calculate the counts for the other error types\n",
    "    spacing_cap_transp_mistake = format_error_details(result4) + format_error_details(out1) + format_error_details(splitted_words)\n",
    "\n",
    "    html_string = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "    <meta charset=\"utf-8\">\n",
    "    <style>\n",
    "        table, th, td {{\n",
    "            border: 1px solid black;\n",
    "            border-collapse: collapse;\n",
    "            font-size: 16px;\n",
    "            font-family: 'Mangal', sans-serif;  /* Specify Mangal as the font family */\n",
    "        }}\n",
    "    </style>\n",
    "    </head>\n",
    "    <body  style=\"padding:15px;\">\n",
    "    <div style=\"text-align:center;\">\n",
    "    <h2 style=\"font-size: 32px;\">Staff Selection Commission</h2>\n",
    "    <h2 style=\"font-size: 32px;\">Typing Test Candidate Report</h2>\n",
    "    </div>\n",
    "\n",
    "    <div>\n",
    "    <p style=\"font-size: 20px;\"><b> Name:****** </b></p>\n",
    "    <p style=\"font-size: 20px;\"><b> Roll Number: {roll_no} </b></p>\n",
    "    <p style=\"font-size: 20px;\"><b> No. of words typed: {words_count1} </b></p>\n",
    "    </div>\n",
    "\n",
    "    <table style=\"width:100%\">\n",
    "    <tr> \n",
    "        <th colspan=\"2\" style=\"background-color: LightSteelBlue;\">Type of Mistakes</th>\n",
    "        <th style=\"background-color: LightSteelBlue;\">No. of Error</th>\n",
    "        <th style=\"background-color: LightSteelBlue;\">Error Detail</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=\"1\"><strong> Full Mistake </strong></td>\n",
    "        <td>Other than Spelling Mistake (Omission/Substitution except transposition/ Addition/ Incomplete Word) </td>\n",
    "        <td> {(total_replaced_words) + (total_missed_words_count) + (total_count_of_added_words)} </td>\n",
    "        <td>{other_than_spelling_mistake}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=\"3\"><strong> Half Mistake </strong></td>\n",
    "        <td>Splitted/Concatnated/Transposition Mistake</td>\n",
    "        <td>{len(out1) + len(result4) + len(splitted_words)}</td>\n",
    "        <td>{spacing_cap_transp_mistake}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Spelling Mistake</td>\n",
    "        <td>{total_spelling_mistakes}</td>\n",
    "        <td>{spelling_mistake}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Punctuation Error</td>\n",
    "        <td>{len(sorted_data)}</td>\n",
    "        <td>{fullstop_err}</td>\n",
    "    </tr>\n",
    "    </table>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    pdf_output_path = r\"C:\\Users\\AnshChoudhary\\Downloads\\Skilltest-2024\\Skilltest-2024\\Evaluated\\3rd Jan- S1\\PDF_Evaluated 3rd Jan- S1\\{}.pdf\".format(roll_no)\n",
    "\n",
    "    options = {\n",
    "        'page-size': 'A4',\n",
    "        'margin-top': '10mm',\n",
    "        'margin-right': '30mm',\n",
    "        'margin-bottom': '10mm',\n",
    "        'margin-left': '0mm',\n",
    "        'encoding': 'UTF-8',\n",
    "    }\n",
    "\n",
    "    pdfkit.from_string(html_string, pdf_output_path, configuration=config, options=options)\n",
    "\n",
    "    # print(\"PDF saved at:\", pdf_output_path)\n",
    "    # print(\"Total time taken:- \")\n",
    "    \n",
    "    return total_spelling_mistakes, H_ERROR_CNT, F_ERROR_CNT, total_mistakes, total_words, mistake_percentage, len(sorted_data)\n",
    "    \n",
    "def candidate_hindi_replacement(file1_path):\n",
    "    with open(file1_path, 'r', encoding='utf-8') as file1:\n",
    "        candidate_text = file1.read()\n",
    "    candidate_text = remove_commasandfullstop(candidate_text)\n",
    "    CwordList = candidate_text.split()\n",
    "    \n",
    "    replacements = {\n",
    "    'рдЕрдд:': 'рдЕрддрдГ',\n",
    "    'рдкреНрд░рд╛рд░рдВрдн': 'рдкреНрд░рд╛рд░рдореНрдн',\n",
    "    'рдЖрд░рдореНрдн': 'рдЖрд░рдВрдореНрдн', \n",
    "    'рдЖрд░рдВрдн': 'рдЖрд░рдВрдореНрдн',\n",
    "    'рдХрд┐рдиреНрддреБ': 'рдХрд┐рдВрддреБ',\n",
    "    'рдмрд╛рдБрдз': 'рдмрд╛рдВрдз',\n",
    "    'рдЙрддреНрддрд░-рдкреНрд░рджреЗрд╢': 'рдЙрддреНрддрд░ рдкреНрд░рджреЗрд╢',\n",
    "    'рдЪреАрдиреА': 'рдЪреАрдиреАрдВ',\n",
    "    'рд▓реЗ рдЬрд╛рдиреЗ': 'рд▓реЗ-рдЬрд╛рдиреЗ',\n",
    "    'рд╢рдд-рдкреНрд░рддрд┐рд╢рдд': 'рд╢рддрдкреНрд░рддрд┐рд╢рдд',\n",
    "    'рд╕реБрд╡рд┐рдзрд╛рдПрдБ': 'рд╕реБрд╡рд┐рдзрд╛рдПрдВ',\n",
    "    'рд╕реБрд╡рд┐рдзрд╛рдРрдВ': 'рд╕реБрд╡рд┐рдзрд╛рдПрдВ',\n",
    "    'рдкрдиреНрджреНрд░рд╣': '15', \n",
    "    'резрел': '15', \n",
    "    'рдкрдВрджреНрд░рд╣': '15',\n",
    "    '2011': 'рджреЛ рд╣рдЬрд╛рд░ рдЧреНрдпрд╛рд░рд╣',\n",
    "    'реирежрезрез': 'рджреЛ рд╣рдЬрд╛рд░ рдЧреНрдпрд╛рд░рд╣',\n",
    "    'рдкрдБрдЪрд╛рдпрддреА': 'рдкрдВрдЪрд╛рдпрддреА',\n",
    "    'рд╕рдБрд╕реНрдерд╛рдиреЛрдВ': 'рд╕рдВрд╕реНрдерд╛рдиреЛрдВ',\n",
    "    '2800': 'рджреЛ рд╣рдЬрд╛рд░ рдЖрда рд╕реМ',\n",
    "    'реиреорежреж': 'рджреЛ рд╣рдЬрд╛рд░ рдЖрда рд╕реМ',\n",
    "    'рдкрдБрдЪрд╛рдпрддреЛрдВ': 'рдкрдВрдЪрд╛рдпрддреЛрдВ',\n",
    "    'рдкрдиреНрджреНрд░рд╣': '15',\n",
    "    'резрел': '15',\n",
    "    'рдЦрдгреНрдбреЛрдВ': 'рдЦрдВрдбреЛрдВ',\n",
    "    'рддреАрди': '3',\n",
    "    'рей': '3',\n",
    "    'рд╕рдореНрдкреВрд░реНрдг': 'рд╕рдВрдкреВрд░реНрдг',\n",
    "    'рдЬрди рдЖрдВрджреЛрд▓рди': 'рдЬрди-рдЖрдВрджреЛрд▓рди',\n",
    "    '2012': 'рджреЛ рд╣рдЬрд╛рд░ рдмрд╛рд░рд╣',\n",
    "    'реирежрезреи': 'рджреЛ рд╣рдЬрд╛рд░ рдмрд╛рд░рд╣',\n",
    "    '1990': 'рдЙрдиреНрдиреАрд╕ рд╕реМ рдирдмреНрдмреЗ',\n",
    "    'резрепрепреж': 'рдЙрдиреНрдиреАрд╕ рд╕реМ рдирдмреНрдмреЗ',\n",
    "    '2010': 'рджреЛ рд╣рдЬрд╛рд░ рджрд╕',\n",
    "    'реирежрезреж': 'рджреЛ рд╣рдЬрд╛рд░ рджрд╕',\n",
    "    'рдПрдХ рджрд╢рдорд▓рд╡ рдЖрда': '1.8',\n",
    "    'рез рджрд╢рдорд▓рд╡ рео': '1.8',\n",
    "    'рез.рео': '1.8',\n",
    "    'рджреЛ рджрд╢рдорд▓рд╡ рдкрд╛рдВрдЪ': '2.5',\n",
    "    'реи рджрд╢рдорд▓рд╡ рел': '2.5',\n",
    "    'реи.рел': '2.5',\n",
    "    'рджрд╕': '10',\n",
    "    'резреж': '10',\n",
    "    # 'рдЪрд╛рд░': '4',\n",
    "    # 'рек': '4',\n",
    "    'рдЖрдБрдХрдбрд╝реЗ': 'рдЖрдВрдХрдбрд╝реЗ',\n",
    "    'рд╕реИрдВрддрд╛рд▓реАрд╕': '47',\n",
    "    'рекрен': '47',\n",
    "    '%': 'рдкреНрд░рддрд┐рд╢рдд',\n",
    "    'рдкреНрд░рддрд┐ рд╢рдд': 'рдкреНрд░рддрд┐рд╢рдд',\n",
    "    'рдкреНрд░рддрд┐-рд╢рдд': 'рдкреНрд░рддрд┐рд╢рдд',\n",
    "    'рдЧреНрдпрд╛рд░рд╣': '11',\n",
    "    'резрез': '11',\n",
    "    'рдорд╛рдирджрдВрдбреЛрдВ': 'рдорд╛рдирджрдгреНрдбреЛрдВ',\n",
    "    'рдЧрдиреНрджрд╛': 'рдЧрдВрджрд╛',\n",
    "    'рд╕реБрд╡рд┐рдзрд╛рдРрдВ': 'рд╕реБрд╡рд┐рдзрд╛рдПрдВ',\n",
    "    'рдЧрд╛рдБрд╡реЛрдВ': 'рдЧрд╛рдВрд╡реЛрдВ',\n",
    "    'рдЧрд╛рдБрд╡': 'рдЧрд╛рдВрд╡',\n",
    "    'рд╕рдбрд╝рд╕рда': '67',\n",
    "    'ремрен': '67',\n",
    "    'рдЫ:': 'рдЫрдГ',\n",
    "    # '6': 'рдЫрдГ',\n",
    "    'рдЫрд╣': 'рдЫрдГ',\n",
    "    'рем': 'рдЫрдГ',\n",
    "    'рд╕рдореНрдмрдзреА': 'рд╕рдВрдмрдВрдзреА',\n",
    "    'рд╕рдВрдмрдиреНрдзреА': 'рд╕рдВрдмрдВрдзреА',\n",
    "    'рд╕рдореНрдмрдиреНрдзреА': 'рд╕рдВрдмрдВрдзреА',\n",
    "    'рдХреЗрдиреНрджреНрд░рд┐рдд': 'рдХреЗрдВрджреНрд░рд┐рдд',\n",
    "    'рдЖрдБрдЧрдирдмрд╛рдбрд╝рд┐рдпреЛрдВ': 'рдЖрдВрдЧрдирдмрд╛рдбрд╝рд┐рдпреЛрдВ',\n",
    "    'рд╢рдд-рдкреНрд░рддрд┐рд╢рдд': 'рд╢рддрдкреНрд░рддрд┐рд╢рдд',\n",
    "    'рд╕рдиреНрджреЗрд╢': 'рд╕рдВрджреЗрд╢',\n",
    "    'рдЬрд╛рдПрдБрдЧреЗ': 'рдЬрд╛рдПрдВрдЧреЗ',\n",
    "    'рдЙрдкрд▓рдмреНрдзрд┐рдпрд╛рдБ': 'рдЙрдкрд▓рдмреНрдзрд┐рдпрд╛рдВ',\n",
    "    'рд╕реНрддрдореНрдн': 'рд╕реНрддрдВрдн',\n",
    "    'рдЪрд╛рд╣реВрдБрдЧреА': 'рдЪрд╛рд╣реВрдВрдЧреА',\n",
    "    'рдврд╛рдБрдЪреЛрдВ': 'рдврд╛рдВрдЪреЛрдВ',\n",
    "    'рдЕрд▓рдЧ рдЕрд▓рдЧ': 'рдЕрд▓рдЧ-рдЕрд▓рдЧ',\n",
    "    'рдкреНрд░реМрджреНрдпреЛрдЧрд┐рдХрд┐рдпреЛрдВ': 'рдкреНрд░реМрджреНрдпреЛрдЧрд┐рдХрд┐рдпреЛрдВ',\n",
    "    'рдирд┐рд░рдиреНрддрд░': 'рдирд┐рд░рдВрддрд░',\n",
    "    'рдврдБрдЧ': 'рдврдВрдЧ',\n",
    "    'рдкрд╕рдБрдж': 'рдкрд╕рдВрдж',\n",
    "    'рд╣реВрдБ': 'рд╣реВрдВ',\n",
    "    'рдЖрдиреНрджреЛрд▓рди': 'рдЖрдВрджреЛрд▓рди',\n",
    "    'рдкреНрд░рдзрд╛рди рдордВрддреНрд░реА': 'рдкреНрд░рдзрд╛рдирдордВрддреНрд░реА',\n",
    "    'рджреВрдБ': 'рджреВрдВ',\n",
    "    'рдореБрдЦреНрдп рдордВрддреНрд░реА': 'рдореБрдЦреНрдпрдордВрддреНрд░реА',\n",
    "    'рд╕реНрд╡рддрдиреНрддреНрд░': 'рд╕реНрд╡рддрдВрддреНрд░',\n",
    "    'рд╕реНрд╡рддреНрддрдВрддреНрд░рддрд╛': 'рд╕реНрд╡рддрдВрддреНрд░рддрд╛',\n",
    "    'рд╕рдореНрднрд╡': 'рд╕рдВрднрд╡',\n",
    "    'рд╕рджреНрднрд╛рд╡': 'рд╕рджреНрднрд╛рд╡',\n",
    "    'рдФрджреНрдпреЛрдЧрд┐рдХ': 'рдФрджреНрдпреЛрдЧрд┐рдХ',\n",
    "    '600 рдХрд░реЛрдбрд╝': 'рдЫрдГ рд╕реМ рдХрд░реЛрдбрд╝',\n",
    "    'ремрежреж рдХрд░реЛрдбрд╝': 'рдЫрдГ рд╕реМ рдХрд░реЛрдбрд╝',\n",
    "    'рем рд╕реМ рдХрд░реЛрдбрд╝': 'рдЫрдГ рд╕реМ рдХрд░реЛрдбрд╝',\n",
    "    'рдЕрдиреНрджрд░': 'рдЕрдВрджрд░',\n",
    "    'рдмрдиреНрдж': 'рдмрдВрдж',\n",
    "    'рдпрд╣рд╛рдБ': 'рдпрд╣рд╛рдВ',\n",
    "    'рд╡рд╣рд╛рдБ': 'рд╡рд╣рд╛рдВ',\n",
    "    'рдкрд░рдиреНрддреБ': 'рдкрд░рдВрддреБ',\n",
    "    'рд╕рд╛рдБрд╕рджреЛрдВ': 'рд╕рд╛рдВрд╕рджреЛрдВ',\n",
    "    'рдЙрджреНрдпреЛрдЧ': 'рдЙрджреНрдпреЛрдЧ',\n",
    "    'рдмрд╛рдкреВрдЬреА': 'рдмрд╛рдкреВ рдЬреА',\n",
    "    'рдЧрд╛рдБрдзреАрдЬреА': 'рдЧрд╛рдВрдзреА рдЬреА',\n",
    "    'рдЧрд╛рдВрдзреАрдЬреА': 'рдЧрд╛рдВрдзреА рдЬреА',\n",
    "    'рдЧрд╛рдБрдзреА рдЬреА': 'рдЧрд╛рдВрдзреА рдЬреА',\n",
    "    'рдЧрд╛рдБрдзреА': 'рдЧрд╛рдВрдзреА'\n",
    "    }\n",
    "\n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'рдЧрд╛рдБрдзреА' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'рдЬреА':\n",
    "            CwordList[idx] = 'рдЧрд╛рдВрдзреА рдЬреА'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "    \n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '6' and (idx + 1) < len(CwordList) and not(CwordList[idx + 1] == 'рд╕реМ' or CwordList[idx + 1] == 'рд╕реМрдВ'):\n",
    "            CwordList[idx] = 'рдЫрдГ'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'рдЪрд╛рд░' and (idx + 1) < len(CwordList) and not CwordList[idx + 1] == 'рджреАрд╡рд╛рд░реА':\n",
    "            CwordList[idx] = '4'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'рек' and (idx + 1) < len(CwordList) and not CwordList[idx + 1] == 'рджреАрд╡рд╛рд░реА':\n",
    "            CwordList[idx] = '4'\n",
    "\n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'рд▓реЗ' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'рдЬрд╛рдиреЗ':\n",
    "            CwordList[idx] = 'рд▓реЗ-рдЬрд╛рдиреЗ'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "        \n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'рдЕрд▓рдЧ' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'рдЕрд▓рдЧ':\n",
    "            CwordList[idx] = 'рдЕрд▓рдЧ-рдЕрд▓рдЧ'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "    \n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word in replacements:\n",
    "            CwordList[idx] = replacements[word]\n",
    "            \n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'реи' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'рджрд╢рдорд▓рд╡' and (idx +2) < len(CwordList) and CwordList[idx + 2] == 'рел':\n",
    "            CwordList[idx] = '2.5'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "            indexes_to_remove.append(idx + 2)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "    \n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'рджреЛ' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'рджрд╢рдорд▓рд╡' and (idx +2) < len(CwordList) and CwordList[idx + 2] == 'рдкрд╛рдВрдЪ':\n",
    "            CwordList[idx] = '2.5'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "            indexes_to_remove.append(idx + 2)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "        \n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'рез' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'рджрд╢рдорд▓рд╡' and (idx +2) < len(CwordList) and CwordList[idx + 2] == 'рео':\n",
    "            CwordList[idx] = '1.8'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "            indexes_to_remove.append(idx + 2)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "        \n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'рдПрдХ' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'рджрд╢рдорд▓рд╡' and (idx +2) < len(CwordList) and CwordList[idx + 2] == 'рдЖрда':\n",
    "            CwordList[idx] = '1.8'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "            indexes_to_remove.append(idx + 2)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "        \n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'рдЬрди' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'рдЖрдВрджреЛрд▓рди':\n",
    "            CwordList[idx] = 'рдЬрди-рдЖрдВрджреЛрд▓рди'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word in replacements:\n",
    "            CwordList[idx] = replacements[word]\n",
    "            \n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '600' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'рдХрд░реЛрдбрд╝':\n",
    "            CwordList[idx] = 'рдЫрдГ рд╕реМ рдХрд░реЛрдбрд╝'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "        \n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'ремрежреж' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'рдХрд░реЛрдбрд╝':\n",
    "            CwordList[idx] = 'рдЫрдГ рд╕реМ рдХрд░реЛрдбрд╝'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "\n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'рем' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'рд╕реМ' and (idx +2) < len(CwordList) and CwordList[idx + 2] == 'рдХрд░реЛрдбрд╝':\n",
    "            CwordList[idx] = 'рдЫрдГ рд╕реМ рдХрд░реЛрдбрд╝'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "            indexes_to_remove.append(idx + 2)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "\n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'рдЫрд╣' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'рд╕реМ' and (idx +2) < len(CwordList) and CwordList[idx + 2] == 'рдХрд░реЛрдбрд╝':\n",
    "            CwordList[idx] = 'рдЫрдГ рд╕реМ рдХрд░реЛрдбрд╝'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "            indexes_to_remove.append(idx + 2)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "        \n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'рдкреНрд░рдзрд╛рди' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'рдордВрддреНрд░реА':\n",
    "            CwordList[idx] = 'рдкреНрд░рдзрд╛рдирдордВрддреНрд░реА'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "\n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'рдореБрдЦреНрдп' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'рдордВрддреНрд░реА':\n",
    "            CwordList[idx] = 'рдореБрдЦреНрдпрдордВрддреНрд░реА'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "\n",
    "    Ctext = ' '.join(CwordList)\n",
    "    Ctext = Ctext.strip()\n",
    "    # print(Ctext)\n",
    "\n",
    "    return Ctext\n",
    "\n",
    "def main(main_pth):\n",
    "    start_time = time.time()\n",
    "    cols = ['asr_rollno', 'asr_region_code', 'asr_date_appeared', 'asr_batch_no', 'asr_spelling_mistakes', 'asr_half_error', 'asr_misc_error', 'asr_punctuation_error','asr_total_mistakes', 'asr_no_of_word_original', 'asr_per_of_error', 'asr_stenograde', 'asr_remarks']\n",
    "    ldc_error = pd.DataFrame(columns= cols)\n",
    "\n",
    "    main_folder = os.listdir(main_pth)\n",
    "    file1_pth = \"\"\n",
    "    file2_pth = \"\"\n",
    "    allowed_spellings_pth = \"\"\n",
    "    asr_remarks = \"Steno-Hindi\"\n",
    "    asr_stenograde = \"Grade-D\"\n",
    "    temp_root = r\"C:\\Users\\AnshChoudhary\\Downloads\\Skilltest-2024\\Skilltest-2024\\Evaluated\\3rd Jan- S1\\temp_text\"\n",
    "    \n",
    "    for root, dirs, files in os.walk(main_pth):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                allowed_spellings_pth = os.path.join(root, file)  \n",
    "                #print(allowed_spellings_pth)  \n",
    "\n",
    "            elif file.endswith(\".txt\"):\n",
    "                if file.lower().startswith(\"master\"):\n",
    "                    ofile2_pth = os.path.join(root, file)\n",
    "\n",
    "                    temp_file2 = candidate_hindi_replacement(ofile2_pth)\n",
    "                    temp_pth = os.path.join(temp_root,\"master.txt\") \n",
    "                    with open(temp_pth, 'w', encoding='utf-8') as file2:\n",
    "                        file2.write(temp_file2)\n",
    "                    \n",
    "                    file2_pth = temp_pth\n",
    "\n",
    "                else:\n",
    "                    ofile1_pth = os.path.join(root, file)\n",
    "                    \n",
    "                    path_lst = ofile1_pth.split('\\\\')\n",
    "                    f = path_lst[-1].split('.')\n",
    "                    asr_rollno = f[-2]\n",
    "                    first_digit = str(asr_rollno)[0]\n",
    "                    if first_digit == '1':\n",
    "                        asr_region_code = 'NWR'\n",
    "                    elif first_digit == '2':\n",
    "                        asr_region_code = 'NR'\n",
    "                    elif first_digit == '3':\n",
    "                        asr_region_code = 'CR'\n",
    "                    elif first_digit == '4':\n",
    "                        asr_region_code = 'ER'\n",
    "                    elif first_digit == '5':\n",
    "                        asr_region_code = 'NCR'\n",
    "                    elif first_digit == '6':\n",
    "                        asr_region_code = 'MPR'\n",
    "                    elif first_digit == '7':\n",
    "                        asr_region_code = 'WR'\n",
    "                    elif first_digit == '8':\n",
    "                        asr_region_code = 'SR'\n",
    "                    elif first_digit == '9':\n",
    "                        asr_region_code = 'KKR'\n",
    "                        \n",
    "                    asr_date_appeared = '03-01-2024'\n",
    "                    asr_batch_no =  'Shift-2'\n",
    "\n",
    "                    temp_file1 = candidate_hindi_replacement(ofile1_pth)\n",
    "                    temp_pth = os.path.join(temp_root,f\"{asr_rollno}.txt\") \n",
    "                    with open(temp_pth, 'w', encoding = 'utf-8') as file1:\n",
    "                        file1.write(temp_file1)\n",
    "\n",
    "                    file1_pth = temp_pth\n",
    "\n",
    "                    total_spelling_mistakes, H_ERROR_CNT, F_ERROR_CNT, total_mistakes, total_words, mistake_percentage,result8 = calculate_mistake_percentage(file1_pth, file2_pth, ofile1_pth, ofile2_pth) \n",
    "                    \n",
    "                    row = {'asr_rollno': asr_rollno,\n",
    "                           'asr_region_code': asr_region_code,\n",
    "                           'asr_date_appeared': asr_date_appeared,\n",
    "                           'asr_batch_no': asr_batch_no,\n",
    "                           'asr_remarks': asr_remarks,\n",
    "                           'asr_stenograde': asr_stenograde,\n",
    "                           'asr_spelling_mistakes': total_spelling_mistakes,\n",
    "                           'asr_half_error': H_ERROR_CNT,\n",
    "                           'asr_misc_error': F_ERROR_CNT,\n",
    "                           'asr_punctuation_error': result8,\n",
    "                           'asr_total_mistakes': total_mistakes,\n",
    "                           'asr_no_of_word_original': 800,\n",
    "                           'asr_per_of_error': mistake_percentage}\n",
    "\n",
    "                    ldc_error = pd.concat([ldc_error, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "    output_excel_path = r\"C:\\Users\\AnshChoudhary\\Downloads\\Skilltest-2024\\Skilltest-2024\\Evaluation\\3rd Jan- S1\\Percentage_report 3rd Jan- S1.xlsx\"\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    \n",
    "    print(total_time)\n",
    "    ldc_error.to_excel(output_excel_path, index=False)\n",
    "\n",
    "main(r\"C:\\Users\\AnshChoudhary\\Downloads\\Skilltest-2024\\Skilltest-2024\\Evaluation\\3rd Jan- S1\") \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d52790c-45b6-4e20-b6b7-7dbe4672fb3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
