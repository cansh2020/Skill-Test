{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3ac0d113-5635-4e32-ab86-c25f80fc11f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\AnshChoudhary\\AppData\\Local\\Temp\\ipykernel_12296\\1915974265.py:1349: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  ldc_error = pd.concat([ldc_error, pd.DataFrame([row])], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5234.635542392731\n"
     ]
    }
   ],
   "source": [
    "import difflib # Used for sequence matching\n",
    "import string # Used for String Operations\n",
    "import re # Used for Regex Operations\n",
    "from difflib import unified_diff\n",
    "from fuzzywuzzy import fuzz\n",
    "import nltk\n",
    "import os \n",
    "import pandas as pd # Used for working on Dataframes\n",
    "import time\n",
    "import os \n",
    "import pandas as pd # Used for working on Dataframes\n",
    "from openpyxl.workbook import Workbook\n",
    "import pdfkit\n",
    "    \n",
    "def get_word_context(words, index, context_size):\n",
    "    start = max(0, index - context_size)\n",
    "    end = min(len(words), index + context_size + 1)\n",
    "    context = ' '.join(words[start:end])\n",
    "    return context.strip()\n",
    "\n",
    "def get_aheadword_context(words, index, context_size):\n",
    "    start = min(len(words), index + 1)\n",
    "    end = min(len(words), index + context_size + 1)\n",
    "    context = ' '.join(words[start:end])\n",
    "    return context.strip()\n",
    "\n",
    "def get_behindword_context(words, index, context_size):\n",
    "    start = max(0, index - context_size)\n",
    "    end = index  # Fix: Set end to index\n",
    "    context = ' '.join(words[start:end])\n",
    "    return context.strip()\n",
    "\n",
    "def remove_commasandfullstop(text):\n",
    "    # Remove commas from the text\n",
    "    cleaned_text = text.replace(',', '').replace('।', '')\n",
    "    \n",
    "    return cleaned_text\n",
    "\n",
    "def singularize_with_spacy(word):\n",
    "    doc = nlp(word)\n",
    "    singularized_words = [token.lemma_ for token in doc]\n",
    "    return singularized_words[0] if singularized_words else word\n",
    "\n",
    "def word_by_word_diff(a, b):\n",
    "    len_a = len(a)\n",
    "    len_b = len(b)\n",
    "    def get_word_context(words, index, context_size):\n",
    "            start = max(0, index - context_size)\n",
    "            end = min(len(words), index + context_size + 1)\n",
    "            context = ' '.join(words[start:end])\n",
    "            return context\n",
    "\n",
    "    # Initialize a matrix to track matches between words\n",
    "    matrix = [[0] * (len_b + 1) for _ in range(len_a + 1)]\n",
    "\n",
    "    # Fill the matrix to identify matching words\n",
    "    for i in range(len_a + 1):\n",
    "        for j in range(len_b + 1):\n",
    "            if i == 0 or j == 0:\n",
    "                matrix[i][j] = 0\n",
    "            elif a[i - 1] == b[j - 1]:\n",
    "                matrix[i][j] = matrix[i - 1][j - 1] + 1\n",
    "            else:\n",
    "                matrix[i][j] = max(matrix[i - 1][j], matrix[i][j - 1])\n",
    "\n",
    "    # Trace the matrix to identify differences\n",
    "    i, j = len_a, len_b\n",
    "    operations = []\n",
    "    while i > 0 or j > 0:\n",
    "        if i > 0 and j > 0 and a[i - 1] == b[j - 1]:\n",
    "            cntxt_similarity = fuzz.ratio(get_word_context(a, i-1, 1), get_word_context(b, j-1, 1))\n",
    "            if cntxt_similarity > 70:\n",
    "                operations.append(('equal', i - 1, j - 1))\n",
    "                i -= 1\n",
    "                j -= 1\n",
    "            else:\n",
    "                operations.append(('insert', j - 1))\n",
    "                j -= 1\n",
    "                operations.append(('delete', i - 1))\n",
    "                i -= 1\n",
    "        else:\n",
    "            if j > 0 and (i == 0 or matrix[i][j - 1] >= matrix[i - 1][j]):\n",
    "                operations.append(('insert', j - 1))\n",
    "                j -= 1\n",
    "            elif i > 0 and (j == 0 or matrix[i][j - 1] < matrix[i - 1][j]):\n",
    "                operations.append(('delete', i - 1))\n",
    "                i -= 1\n",
    "\n",
    "    # Reverse the operations to maintain order\n",
    "    operations.reverse()\n",
    "\n",
    "    # Generate the word list with tags\n",
    "    output = []\n",
    "    for op in operations:\n",
    "        if op[0] == 'equal':\n",
    "            output.append((f' {a[op[1]]}'))\n",
    "        elif op[0] == 'insert':\n",
    "            output.append((f'+{b[op[1]]}'))\n",
    "        elif op[0] == 'delete':\n",
    "            output.append((f'-{a[op[1]]}'))\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "# Function to evaluate mistakes\n",
    "def Numberof_mistakes(Mtext, Ctext):\n",
    "\n",
    "    c = 0  # Count for substituted words\n",
    "    y = 0  # Count for misspelled words\n",
    "\n",
    "    # Split candidate text into sentences\n",
    "    sentences = nltk.sent_tokenize(Ctext)\n",
    "    Titlerror_List=[]  # List to hold words with title case errors\n",
    "\n",
    "    Mtext = remove_commasandfullstop(Mtext)\n",
    "    Ctext = remove_commasandfullstop(Ctext)\n",
    "    \n",
    "    MwordList = Mtext.split()\n",
    "    MwordList.pop(0)\n",
    "    \n",
    "    CwordList = Ctext.split()\n",
    "    CwordList = [word.replace('\\u200d', '') for word in CwordList]\n",
    "    \n",
    "    master_Wtotal = len(MwordList)\n",
    "    candidate_Wtotal = len(CwordList)\n",
    "    \n",
    "    differ = list(unified_diff(MwordList, CwordList))\n",
    "    segment_lenM = []\n",
    "    segment_lenC = []\n",
    "\n",
    "    for line in differ:\n",
    "        if line.startswith('@'):\n",
    "            a_match = re.search(r'-(\\d+),', line)\n",
    "            b_match = re.search(r'\\+(\\d+),', line)\n",
    "            \n",
    "            a = int(a_match.group().split(',')[0]) if a_match else 0\n",
    "            b = int(b_match.group().split(',')[0]) if b_match else 0\n",
    "\n",
    "            segment_lenM.append(abs(a))\n",
    "            segment_lenC.append(b)\n",
    "\n",
    "    M_chunks = []\n",
    "    start = 0\n",
    "\n",
    "    for length in segment_lenM:\n",
    "        M_chunks.append(MwordList[start:length])\n",
    "        start = length\n",
    "\n",
    "    M_chunks.append(MwordList[start:]) # Adding the remaining elements to the last list\n",
    "\n",
    "    C_chunks = []\n",
    "    start = 0\n",
    "\n",
    "    for length in segment_lenC:\n",
    "        C_chunks.append(CwordList[start:length])\n",
    "        start = length\n",
    "\n",
    "    C_chunks.append(CwordList[start:]) # Adding the remaining elements to the last list\n",
    "\n",
    "    differ = []\n",
    "\n",
    "    inx = 0\n",
    "    # Compare each chunk separately\n",
    "    while inx < len(M_chunks):\n",
    "        diff = list(word_by_word_diff(C_chunks[inx], M_chunks[inx]))\n",
    "        if diff:\n",
    "            for line in diff:\n",
    "                differ.append(line)\n",
    "        inx+=1\n",
    "\n",
    "    M_index = 0\n",
    "    C_index = 0\n",
    "    t = 0\n",
    "    tupple=[]\n",
    "    omitted=[]\n",
    "    added=[]\n",
    "    l = 0\n",
    "\n",
    "    # similarity = fuzz.ratio('ansh.', 'ansh,')\n",
    "    # print(similarity)\n",
    "\n",
    "    mastertext = [element for element in differ if not element.startswith('+')]\n",
    "    candidatetext = [element for element in differ if not element.startswith('-')]\n",
    "    \n",
    "    for i, item in enumerate(differ):\n",
    "        if i >= l:\n",
    "            if item.startswith('-'):  # Checks if the item in the 'differ' list represents a deletion in the master text\n",
    "                j = i\n",
    "                check1 = []\n",
    "                check2 = []\n",
    "                while j < len(differ) and not differ[j].startswith(' '):\n",
    "                    if differ[j].startswith('-'):\n",
    "                        word1 = differ[j]  # Extracts the word that is deleted in the master text\n",
    "                        # Locate word1 in the master text\n",
    "                        while M_index < len(MwordList) and not mastertext[M_index] == word1:\n",
    "                            M_index += 1  # Moves through the master text word list until it finds 'word1'\n",
    "\n",
    "                        word1 = word1[1:]\n",
    "                        check1.append((word1, M_index))\n",
    "                        M_index += 1\n",
    "                    if differ[j].startswith('+'):\n",
    "                        word2 = differ[j] # Extracts the word that is added in the candidate text\n",
    "\n",
    "                        # Locate word2 in the candidate text\n",
    "                        while C_index < len(CwordList) and not candidatetext[C_index] == word2:\n",
    "                            C_index += 1  # Moves through the candidate text word list until it finds 'word2'\n",
    "\n",
    "                        word2 = word2[1:]\n",
    "                        check2.append((word2, C_index))\n",
    "                        C_index += 1\n",
    "                    j += 1\n",
    "                    l = j\n",
    "                m = 0\n",
    "                # print(check2)\n",
    "                # print(check1)\n",
    "\n",
    "                all_matched_words = []\n",
    "                replaced_words = []\n",
    "                for y, tuple2 in enumerate(check2):\n",
    "                    element2, index2 = tuple2\n",
    "                    empty = []\n",
    "                    for x, tuple1 in enumerate(check1):\n",
    "                        element1, index1 = tuple1\n",
    "                        if x>=m:\n",
    "                            similarity = fuzz.ratio(element2, element1)\n",
    "                            ahead_similarity = fuzz.ratio(get_aheadword_context(MwordList, index2, 10), get_aheadword_context(CwordList, index1, 10))\n",
    "                            behind_similarity = fuzz.ratio(get_behindword_context(MwordList, index2, 10), get_behindword_context(CwordList, index1, 10))\n",
    "                            if get_word_context(MwordList, index2, 1) and get_word_context(CwordList, index1, 1):\n",
    "                                cntxt_similarity = fuzz.ratio(get_word_context(MwordList, index2, 1), get_word_context(CwordList, index1, 1))\n",
    "                                if similarity > 60 and cntxt_similarity>60:\n",
    "                                    empty.append((((element2, index2), (element1, index1)), similarity, x))\n",
    "                                elif similarity == 100 and (ahead_similarity>70 or behind_similarity>70):\n",
    "                                    empty.append((((element2, index2), (element1, index1)), similarity, x))\n",
    "                                elif similarity > 65 and (ahead_similarity>70 or behind_similarity>70):\n",
    "                                    empty.append((((element2, index2), (element1, index1)), similarity, x))\n",
    "\n",
    "                    sorted_empty = sorted(empty, key=lambda x: x[1], reverse=True)  # Sort based on the second element of each tuple\n",
    "                    if len(sorted_empty)>0:\n",
    "                        # tupple.append(sorted_empty[0][0])\n",
    "                        all_matched_words.append((sorted_empty[0][0], y, sorted_empty[0][2]))\n",
    "                        replaced_words.append((y, sorted_empty[0][2]))\n",
    "                        m = sorted_empty[0][2] + 1\n",
    "\n",
    "                for i, x in enumerate(all_matched_words):\n",
    "                    if (i+1) in range(len(all_matched_words)):\n",
    "                        midx1 = x[1]\n",
    "                        cidx1 = x[2]\n",
    "                        midx2 = all_matched_words[i+1][1]\n",
    "                        cidx2 = all_matched_words[i+1][2]\n",
    "\n",
    "                        if (midx2 - midx1) > 0 and (cidx2 - cidx1) > 0:\n",
    "                            mrange = [m for m in range((midx1 + 1), midx2)]\n",
    "                            crange = [n for n in range((cidx1 + 1), cidx2)]\n",
    "\n",
    "                            # Pair elements from mrange and crange\n",
    "                            paired_elements = list(zip(mrange, crange))\n",
    "                            for pair in paired_elements:\n",
    "                                if len(pair)>0:\n",
    "                                    replaced_words.append(pair)\n",
    "                    else:\n",
    "                        midx1 = x[1]\n",
    "                        cidx1 = x[2]\n",
    "\n",
    "                        if (len(check2) -1 -midx1) > 0 and (len(check1) -1 - cidx1) > 0:\n",
    "                            mrange = [m for m in range((midx1 + 1), (len(check2)))]\n",
    "                            crange = [n for n in range((cidx1 + 1), (len(check1)))]\n",
    "\n",
    "                            # Pair elements from mrange and crange\n",
    "                            paired_elements = list(zip(mrange, crange))\n",
    "                            for pair in paired_elements:\n",
    "                                if len(pair)>0:\n",
    "                                    replaced_words.append(pair)\n",
    "                if len(all_matched_words)>0:\n",
    "                    if all_matched_words[0][1]>0 and all_matched_words[0][2]>0:\n",
    "                        midx2 = all_matched_words[0][1]\n",
    "                        cidx2 = all_matched_words[0][2]\n",
    "\n",
    "                        if (midx2) > 0 and (cidx2) > 0:\n",
    "                            mrange = [m for m in range(0, midx2)]\n",
    "                            crange = [n for n in range(0, cidx2)]\n",
    "                            \n",
    "                            # Pair elements from mrange and crange\n",
    "                            paired_elements = list(zip(mrange, crange))\n",
    "                            for pair in paired_elements:\n",
    "                                if len(pair)>0:\n",
    "                                    replaced_words.append(pair)\n",
    "\n",
    "                if len(all_matched_words)==0:\n",
    "                    mrange = [m for m in range(0, len(check2))]\n",
    "                    crange = [n for n in range(0, len(check1))]\n",
    "                    # Pair elements from mrange and crange\n",
    "                    paired_elements = list(zip(mrange, crange))\n",
    "                    for pair in paired_elements:\n",
    "                        if len(pair)>0:\n",
    "                            replaced_words.append(pair)\n",
    "\n",
    "                replaced_words = sorted(replaced_words, key=lambda x:x[0])\n",
    "                for one in replaced_words:\n",
    "                    a1, a2 = one\n",
    "                    tupple.append((check2[a1], check1[a2]))\n",
    "\n",
    "                empty = []\n",
    "                if len(check2)>0:\n",
    "                    for om_i, om_tup in enumerate(check2):\n",
    "                        for tu in replaced_words:\n",
    "                            mi = tu[0]\n",
    "                            empty.append(mi)\n",
    "\n",
    "                        if om_i not in empty:\n",
    "                            omitted.append(om_tup)\n",
    "\n",
    "                empty = []\n",
    "                if len(check1)>0:\n",
    "                    for ad_i, ad_tup in enumerate(check1):\n",
    "                        for tu in replaced_words:\n",
    "                            ci = tu[1]\n",
    "                            empty.append(ci)\n",
    "\n",
    "                        if ad_i not in empty:\n",
    "                            added.append(ad_tup)\n",
    "\n",
    "            elif item.startswith(' '):\n",
    "                M_index += 1\n",
    "                C_index += 1\n",
    "            elif item.startswith('+'):\n",
    "                omitted.append((item[1:], C_index))\n",
    "                C_index += 1\n",
    "                \n",
    "    a = len(omitted)  # Omitted words count\n",
    "    b = len(added)  # Added words count\n",
    "    y = 0\n",
    "    Mispelled_List = []  # List to hold misspelled words\n",
    "    Substitute = []  # List to hold substituted \n",
    "    for inx, tup in enumerate(tupple):\n",
    "        Mw, Cw = tup\n",
    "        if Mw[0] != Cw[0]:\n",
    "            similarity = fuzz.ratio(Mw[0], Cw[0])\n",
    "            if similarity > 65 :#and (min((len(Mw[0])), len(Cw[0]))/max(len(Mw[0]), len(Cw[0])))> 0.5:  # Check if the candidate word is a misspelling of the master word\n",
    "                Mispelled_List.append(tup)  # Collect misspelled words\n",
    "                y+=1\n",
    "            else:\n",
    "                c += 1\n",
    "                Substitute.append(tup)  # Collect wrongly substituted words\n",
    "\n",
    "    for i, tup in enumerate(Mispelled_List):\n",
    "        p, q = tup\n",
    "        p1, p2 = p\n",
    "        q1, q2 = q\n",
    "        Mispelled_List[i] = (p1, q1, p2, q2)\n",
    "        \n",
    "    intex_to_delete = []\n",
    "    for i, tup in enumerate(Substitute):\n",
    "        p, q = tup\n",
    "        p1, p2 = p\n",
    "        q1, q2 = q\n",
    "        if p1 == '11' and q1 == 'ग्यारह':\n",
    "            intex_to_delete.append(i)\n",
    "        if p1 == '10' and q1 == 'दस':\n",
    "            intex_to_delete.append(i)\n",
    "        if p1== 'ओर' and q1 == 'और' and p2 == 726:\n",
    "            intex_to_delete.append(i)\n",
    "        Substitute[i] = (p1, q1, p2, q2)\n",
    "        \n",
    "    intex_to_delete.sort(reverse=True)\n",
    "    for idx in intex_to_delete:\n",
    "        Substitute.pop(idx)\n",
    "\n",
    "    return a, b, c, y, omitted, added, Mispelled_List, Substitute\n",
    "\n",
    "def compare_punctuation_latest(file1_path, file2_path):\n",
    "\n",
    "    def has_punctuation(word):\n",
    "        return any(char in '।' for char in word)\n",
    "\n",
    "    def is_char_punctuation(character):\n",
    "        if character in '।':\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def get_word_context(words, index, context_size):\n",
    "        start = max(0, index - context_size)\n",
    "        end = min(len(words), index + context_size + 1)\n",
    "        context = ' '.join(words[start:end])\n",
    "        return context\n",
    "\n",
    "    with open(file1_path, 'r', encoding = 'utf-8') as file1:\n",
    "        candidate_txt = file1.read()\n",
    "\n",
    "    with open(file2_path, 'r', encoding = 'utf-8') as file2:\n",
    "        master_txt = file2.read()\n",
    "\n",
    "    candidate_words = candidate_txt.split()\n",
    "    master_words = master_txt.split()\n",
    "    results = []\n",
    "    words_with_punct_master = []\n",
    "    words_with_punct_candidate = []\n",
    "\n",
    "    for mindx, mstr_wrd in enumerate(master_words):\n",
    "        if has_punctuation(mstr_wrd):\n",
    "            words_with_punct_master.append((mstr_wrd, mindx))\n",
    "\n",
    "    #print(\"_________________________MASTER WORDS_____________________\")\n",
    "    for cindx, cand_wrd in enumerate(candidate_words):\n",
    "        if has_punctuation(cand_wrd):\n",
    "            words_with_punct_candidate.append((cand_wrd, cindx))\n",
    "\n",
    "    for mtup in words_with_punct_master:\n",
    "        m_context = get_word_context(master_words, mtup[1], 2)\n",
    "        best_match_context = ''\n",
    "        match_rtio = 0\n",
    "        cand_word_detail = ''\n",
    "        for cnd_idx, cnd_wrd in enumerate(candidate_words):\n",
    "            c_context = get_word_context(candidate_words, cnd_idx, 2)\n",
    "            ratio = fuzz.ratio(m_context.lower(), c_context.lower())\n",
    "            if ratio>match_rtio:\n",
    "                best_match_context = c_context\n",
    "                match_rtio = ratio\n",
    "                cand_word_detail = (mtup[0], mtup[1],  cnd_wrd, cnd_idx)\n",
    "       # print(m_context, \"==||==\", best_match_context, \"==|==\",mtup, \"==|==\",match_rtio)\n",
    "        for charactr in mtup[0]:\n",
    "            if is_char_punctuation(charactr):\n",
    "                if charactr not in best_match_context and match_rtio>70:\n",
    "                    if mtup not in results:    \n",
    "                        punct_mark = ''.join(c for c in mtup[0] if not c.isalnum())\n",
    "                        results.append(f\"{{{mtup[0]}, {mtup[1]}, ({punct_mark}), Punctuation Missed}};\")\n",
    "                elif mtup[0] not in best_match_context:\n",
    "                    match_found = 0\n",
    "                    for wrd in best_match_context.split():\n",
    "                        if fuzz.ratio(mtup[0].lower(), wrd.lower())>70:\n",
    "                            match_found = 1\n",
    "                            break\n",
    "                    if match_found==0:\n",
    "                        if mtup not in results:\n",
    "                            punct_mark = ''.join(c for c in mtup[0] if not c.isalnum())\n",
    "                            results.append(f\"{{{mtup[0]}, {mtup[1]}, ({punct_mark}), Punctuation Missed}};\")\n",
    "\n",
    "\n",
    "    for ctup in words_with_punct_candidate:\n",
    "        cand_context = get_word_context(candidate_words, ctup[1], 2)\n",
    "        best_mtch_context = ''\n",
    "        mtch_rtio = 0\n",
    "        mstr_word_detail = ''\n",
    "        for mstr_idx, master_wrd in enumerate(master_words):\n",
    "            mster_context = get_word_context(master_words, mstr_idx, 2)\n",
    "            mtch_ratio = fuzz.ratio(mster_context.lower(), cand_context.lower())\n",
    "            if mtch_ratio>mtch_rtio:\n",
    "                best_mtch_context = mster_context\n",
    "                mtch_rtio = mtch_ratio\n",
    "                mstr_word_detail = (ctup[0], ctup[1],  master_wrd, mstr_idx)\n",
    "        for charctr in ctup[0]:\n",
    "            if is_char_punctuation(charctr):\n",
    "                if charctr not in best_mtch_context:\n",
    "                    if ctup not in results:\n",
    "                        punct_mark = ''.join(c for c in ctup[0] if not c.isalnum())\n",
    "                        results.append(f\"{{{ctup[0]}, {ctup[1]}, ({punct_mark}), Punctuation Added}};\")\n",
    "                        \n",
    "    return results\n",
    "\n",
    "def Splconandtrans(file2_path, file1_path):\n",
    "    with open(file2_path, 'r', encoding='utf-8') as file2:\n",
    "        Mtext = file2.read()\n",
    "\n",
    "    with open(file1_path, 'r', encoding='utf-8') as file1:\n",
    "        Ctext = file1.read()\n",
    "\n",
    "    c = 0  # Count for substituted words\n",
    "    y = 0  # Count for misspelled words\n",
    "\n",
    "    Mtext = remove_commasandfullstop(Mtext)\n",
    "    Ctext = remove_commasandfullstop(Ctext)\n",
    "\n",
    "    MwordList = Mtext.split()\n",
    "    MwordList.pop(0)\n",
    "    \n",
    "    CwordList = Ctext.split()\n",
    "    CwordList = [word.replace('\\u200d', '') for word in CwordList]\n",
    "\n",
    "    master_Wtotal = len(MwordList)\n",
    "    candidate_Wtotal = len(CwordList)\n",
    "\n",
    "    differ = list(unified_diff(MwordList, CwordList))\n",
    "    segment_lenM = []\n",
    "    segment_lenC = []\n",
    "\n",
    "    for line in differ:\n",
    "        if line.startswith('@'):\n",
    "            a_match = re.search(r'-(\\d+),', line)\n",
    "            b_match = re.search(r'\\+(\\d+),', line)\n",
    "            \n",
    "            a = int(a_match.group().split(',')[0]) if a_match else 0\n",
    "            b = int(b_match.group().split(',')[0]) if b_match else 0\n",
    "\n",
    "            segment_lenM.append(abs(a))\n",
    "            segment_lenC.append(b)\n",
    "\n",
    "    M_chunks = []\n",
    "    start = 0\n",
    "\n",
    "    for length in segment_lenM:\n",
    "        M_chunks.append(MwordList[start:length])\n",
    "        start = length\n",
    "\n",
    "    M_chunks.append(MwordList[start:]) # Adding the remaining elements to the last list\n",
    "\n",
    "    C_chunks = []\n",
    "    start = 0\n",
    "\n",
    "    for length in segment_lenC:\n",
    "        C_chunks.append(CwordList[start:length])\n",
    "        start = length\n",
    "\n",
    "    C_chunks.append(CwordList[start:]) # Adding the remaining elements to the last list\n",
    "\n",
    "    differ = []\n",
    "\n",
    "    inx = 0\n",
    "    # Compare each chunk separately\n",
    "    while inx < len(M_chunks):\n",
    "        diff = list(word_by_word_diff(C_chunks[inx], M_chunks[inx]))\n",
    "        if diff:\n",
    "            for line in diff:\n",
    "                differ.append(line)\n",
    "        inx+=1\n",
    "\n",
    "    M_index = 0\n",
    "    C_index = 0\n",
    "    t = 0\n",
    "    tupple=[]\n",
    "    omitted=[]\n",
    "    added=[]\n",
    "    l = 0\n",
    "\n",
    "    # similarity = fuzz.ratio('ansh.', 'ansh,')\n",
    "    # print(similarity)\n",
    "\n",
    "    mastertext = [element for element in differ if not element.startswith('+')]\n",
    "    candidatetext = [element for element in differ if not element.startswith('-')]\n",
    "    \n",
    "    splits = []\n",
    "    concat = []\n",
    "    transposed = []\n",
    "    for i, item in enumerate(differ):\n",
    "        if i >= l:\n",
    "            if item.startswith('-'):  # Checks if the item in the 'differ' list represents a deletion in the master text\n",
    "                j = i\n",
    "                check1 = []\n",
    "                check2 = []\n",
    "                while j < len(differ) and not differ[j].startswith(' '):\n",
    "                    if differ[j].startswith('-'):\n",
    "                        word1 = differ[j]  # Extracts the word that is deleted in the master text\n",
    "                        # Locate word1 in the master text\n",
    "                        while M_index < len(MwordList) and not mastertext[M_index] == word1:\n",
    "                            M_index += 1  # Moves through the master text word list until it finds 'word1'\n",
    "\n",
    "                        word1 = word1[1:]\n",
    "                        check1.append((word1, M_index))\n",
    "                        M_index += 1\n",
    "                    if differ[j].startswith('+'):\n",
    "                        word2 = differ[j] # Extracts the word that is added in the candidate text\n",
    "\n",
    "                        # Locate word2 in the candidate text\n",
    "                        while C_index < len(CwordList) and not candidatetext[C_index] == word2:\n",
    "                            C_index += 1  # Moves through the candidate text word list until it finds 'word2'\n",
    "\n",
    "                        word2 = word2[1:]\n",
    "                        check2.append((word2, C_index))\n",
    "                        C_index += 1\n",
    "                    j += 1\n",
    "                    l = j\n",
    "                # print(check2)\n",
    "                # print(check1)\n",
    "                extra = 0\n",
    "                for ind, mt in enumerate(check2):\n",
    "                     element2, index2 = mt\n",
    "                     for indi, cd in enumerate(check1):\n",
    "                        if indi >= extra:\n",
    "                            element1n1, index1n1 = cd\n",
    "                            if (indi+1)<len(check1):\n",
    "                                    element1n2, index1n2 = check1[indi+1]\n",
    "                                    if element2 == f'{element1n1}{element1n2}':\n",
    "                                        out = f'{{{element2}, {element1n1} {element1n2}, {index2}, {index1n1}, Splitted Word}}'\n",
    "                                        splits.append(out)\n",
    "                                        extra = indi+1\n",
    "                                    elif element2 == f'{element1n1}-{element1n2}':\n",
    "                                        out = f'{{{element2}, {element1n1} {element1n2}, {index2}, {index1n1}, Splitted Word}}'\n",
    "                                        splits.append(out)\n",
    "                                        extra = indi+1\n",
    "                                        \n",
    "                extra = 0\n",
    "                for ind, mt in enumerate(check1):\n",
    "                     element1, index1 = mt\n",
    "                     for indi, cd in enumerate(check2):\n",
    "                        if indi >= extra:\n",
    "                            element2n1, index2n1 = cd\n",
    "                            if (indi+1)<len(check2):\n",
    "                                    element2n2, index2n2 = check2[indi+1]\n",
    "                                    if element1 == f'{element2n1}{element2n2}':\n",
    "                                        out = f'{{{element2n1} {element2n2}, {element1}, {index2n1}, {index1}, concatnated Word}}'\n",
    "                                        concat.append(out)\n",
    "                                        extra = indi+1\n",
    "                                    elif element1 == f'{element2n1}-{element2n2}':\n",
    "                                        out = f'{{{element2n1} {element2n2}, {element1}, {index2n1}, {index1}, concatnated Word}}'\n",
    "                                        concat.append(out)\n",
    "                                        extra = indi+1\n",
    "                                        \n",
    "                extra = 0\n",
    "                for ind, mt in enumerate(check2):\n",
    "                    element2n1, index2n1 = mt\n",
    "                    if (ind+1)<len(check2):\n",
    "                        element2n2, index2n2 = check2[ind+1]\n",
    "                        for indi, cd in enumerate(check1):\n",
    "                            if indi >= extra:\n",
    "                                element1n1, index1n1 = cd\n",
    "                                if (indi+1)<len(check1):\n",
    "                                        element1n2, index1n2 = check1[indi+1]\n",
    "                                        if element2n1 == element1n2 and element2n2 == element1n1:\n",
    "                                            out = f'{{{element2n1} {element2n2}, {element1n1} {element1n2}, {index2n1}, {index1n1}, Transposed Word}}'\n",
    "                                            transposed.append(out)\n",
    "                                            extra = indi+1\n",
    "\n",
    "            elif item.startswith(' '):\n",
    "                M_index += 1\n",
    "                C_index += 1\n",
    "            elif item.startswith('+'):\n",
    "                added.append((item[1:], C_index))\n",
    "                C_index += 1\n",
    "                \n",
    "    return splits, concat, transposed\n",
    "\n",
    "def find_fullstop_errors(file1_path, file2_path):\n",
    "    errors = []\n",
    "\n",
    "    def count_fullstops(word):\n",
    "        return word.count('।')\n",
    "\n",
    "    def count_indices(word_file):\n",
    "        with open(word_file, 'r', encoding='utf-8') as file:\n",
    "            content = file.read()\n",
    "        words_file = re.findall(r'[ऀ-ॿ][\\u0900-\\u097F]+', content)\n",
    "        return words_file\n",
    "\n",
    "    word_file1 = count_indices(file1_path)\n",
    "    word_file2 = count_indices(file2_path)    \n",
    "\n",
    "            \n",
    "    def find_word_indices(word_file):\n",
    "        word_indices = {}\n",
    "        for i, word in enumerate(word_file):\n",
    "            if word.endswith('।'):\n",
    "                word_lower = word.lower()\n",
    "                if word_lower not in word_indices:\n",
    "                    word_indices[word_lower] = []\n",
    "                word_indices[word_lower].append(i)\n",
    "        return word_indices\n",
    "\n",
    "    word_indices1 = find_word_indices(word_file1)\n",
    "    word_indices2 = find_word_indices(word_file2)\n",
    "    \n",
    "    all_valuesM = [value for sublist in word_indices2.values() for value in sublist]\n",
    "    all_valuesC = [value for sublist in word_indices1.values() for value in sublist]\n",
    "    all_valuesM = sorted(all_valuesM)\n",
    "    all_valuesC = sorted(all_valuesC)\n",
    "    # print(all_valuesM)\n",
    "    # print(all_valuesC)\n",
    "    master_found = []\n",
    "    candi_found = []\n",
    "    extra = 0\n",
    "    for i2, value2 in enumerate(all_valuesM):\n",
    "        for i1, value1 in enumerate(all_valuesC):\n",
    "            if i1>=extra:\n",
    "                similarityB = fuzz.ratio(get_behindword_context(word_file2, value2, 8), get_behindword_context(word_file1, value1, 8))\n",
    "                similarityA = fuzz.ratio(get_aheadword_context(word_file2, value2, 8), get_aheadword_context(word_file1, value1, 8))\n",
    "                # print(get_behindword_context(word_file2, value2, 8),value2 , get_behindword_context(word_file1, value1, 8), value1, similarityB)\n",
    "                # print(get_aheadword_context(word_file2, value2, 8), value2, get_aheadword_context(word_file1, value1, 8), value1, similarityA)\n",
    "                if similarityB > 75 or similarityA > 75:\n",
    "                    master_found.append(value2)\n",
    "                    candi_found.append(value1)\n",
    "                    extra = i1 + 1\n",
    "                    break\n",
    "    # print(master_found)\n",
    "    # print(candi_found)\n",
    "    for i in candi_found:\n",
    "        result = count_fullstops(word_file1[i])\n",
    "        if result > 1:\n",
    "            error_info = [word_file1[i], '99999', i, 'unnecessary fullstop']\n",
    "            errors.extend([error_info] * (result-1))\n",
    "        \n",
    "    indices2 = [value for value in all_valuesM if value not in master_found]\n",
    "    indices1 = [value for value in all_valuesC if value not in candi_found]\n",
    "    \n",
    "    for i in indices1:\n",
    "        result = count_fullstops(word_file1[i])\n",
    "        error_info = [word_file1[i], '99999', i, 'unnecessary fullstop']\n",
    "        if result> 1:\n",
    "            errors.extend([error_info] * (result-1))\n",
    "        else:\n",
    "            errors.extend([error_info])\n",
    "    for i in indices2:\n",
    "        errors.extend([[word_file2[i], i, '99999', 'missing fullstop']])\n",
    "        \n",
    "    idx = []    \n",
    "    for i in errors:\n",
    "        if i[3].startswith('miss'):\n",
    "            idx.append(i[1])\n",
    "        elif i[3].startswith('unn'):\n",
    "            idx.append(i[2])\n",
    "        idx = sorted(idx)\n",
    "    sorted_data = sorted(errors, key=lambda x: idx.index(x[1]) if x[3] == 'missing fullstop' else idx.index(x[2]))\n",
    "    sorted_data_str = \"{\" + \"}, {\".join(\", \".join(str(i) for i in item) for item in sorted_data) + \"}\"\n",
    "\n",
    "    return sorted_data, sorted_data_str\n",
    "\n",
    "def calculate_mistake_percentage(file1_path, file2_path, ofile1_pth, ofile2_pth):\n",
    "    def count_words(file_path):\n",
    "        with open(file_path, 'r', encoding='utf-8') as words_file:\n",
    "            words_text = words_file.read()\n",
    "            words_list = words_text.split()\n",
    "            \n",
    "        return len(words_list)\n",
    "\n",
    "    def roll_no(file_path):\n",
    "        file_name = os.path.splitext(os.path.basename(file_path))[0]\n",
    "        return file_name\n",
    "    \n",
    "    total_words = 0\n",
    "    full_mistakes = 0\n",
    "    half_mistakes = 0\n",
    "\n",
    "    result5, out1, result4 = Splconandtrans(file2_path, file1_path)\n",
    "    if out1:\n",
    "        half_mistakes += len(out1)\n",
    "    \n",
    "    if result5:\n",
    "        half_mistakes += len(result5)\n",
    "    # print(result5)\n",
    "    if result4:\n",
    "        half_mistakes += len(result4)\n",
    "\n",
    "    with open(file2_path, 'r', encoding = 'utf-8') as mfile:\n",
    "        master_text = mfile.read()\n",
    "    with open(file1_path, 'r', encoding = 'utf-8') as cfile:\n",
    "        candidate_text = cfile.read()\n",
    "\n",
    "    candidate_text = remove_commasandfullstop(candidate_text)\n",
    "    splitted_ctext = candidate_text.split()\n",
    "    words_to_replace = []\n",
    "    for x in out1:\n",
    "        xlist = x.split(', ')\n",
    "        splitted_ctext[int(xlist[-2])] = xlist[0][1:]\n",
    "        \n",
    "    for x in result4:\n",
    "        xlist = x.split(', ')\n",
    "        words = xlist[0].split()\n",
    "        splitted_ctext[int(xlist[-2])] = words[0][1:]\n",
    "        splitted_ctext[int(xlist[-2]) + 1] = words[1]\n",
    "    # print(splitted_ctext)\n",
    "    # print(missing_splits)\n",
    "    indexes_to_remove = []\n",
    "    for x in result5:\n",
    "        xlist = x.split(', ')\n",
    "        splitted_ctext[int(xlist[-2])] = xlist[0][1:]\n",
    "        indexes_to_remove.append(int(xlist[-2])+1)\n",
    "        \n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        splitted_ctext.pop(idx)\n",
    "    \n",
    "        \n",
    "    candidate_text = ' '.join(splitted_ctext)\n",
    "    candidate_text = candidate_text.strip()\n",
    "    candidate_text = candidate_text.lower()\n",
    "    master_text = remove_commasandfullstop(master_text)\n",
    "    master_text = master_text.lower()\n",
    "\n",
    "    a, b, c, z, missed_words, added_words, misspelled, replaced_words = Numberof_mistakes(master_text, candidate_text)\n",
    "\n",
    "    final_spelling_mistakes = ''\n",
    "    total_spelling_mistakes = 0 \n",
    "    for mstk in misspelled:\n",
    "        final_spelling_mistakes=final_spelling_mistakes+'{'+mstk[0]+', '+mstk[1]+', '+str(mstk[2])+', '+str(mstk[3])+', Spelling Mistake}; '\n",
    "        total_spelling_mistakes+=1\n",
    "\n",
    "    errors = []\n",
    "    total_replaced_words = 0 \n",
    "    for repl_wrds in replaced_words:\n",
    "        errors.extend([[repl_wrds[0], repl_wrds[1], repl_wrds[2], repl_wrds[3], 'Replaced Word']])\n",
    "        total_replaced_words+=1\n",
    "\n",
    "    #FINAL MISSED WORDS ARE STORED IN BELOW LIST\n",
    "    total_missed_words_count = 0\n",
    "    for msd_wrds in missed_words:\n",
    "        errors.extend([[msd_wrds[0], msd_wrds[1], '99999', 'Missed']])\n",
    "        total_missed_words_count+=1\n",
    "        \n",
    "    #Filter those extra added words which is already taken in spelling mistakes and replace words mistake\n",
    "    total_count_of_added_words = 0\n",
    "    for extr_add_wrd in added_words:\n",
    "        errors.extend([[extr_add_wrd[0], '99999', extr_add_wrd[1], 'Added']])\n",
    "        total_count_of_added_words+=1\n",
    "        \n",
    "    idx = []\n",
    "    for i in errors:\n",
    "        # print(i)\n",
    "        if len(i)==4:\n",
    "            if 'Missed' in i[3]:\n",
    "                idx.append(i[1])\n",
    "            elif 'Added' in i[3]:\n",
    "                idx.append(i[2])\n",
    "        else:\n",
    "            idx.append(i[2])\n",
    "        idx = sorted(idx)\n",
    "        \n",
    "    sorted_data_full = []\n",
    "    for error in errors:\n",
    "        if len(error)==4:\n",
    "            if error[3] == 'Missed':\n",
    "                sorted_data_full.append((error, idx.index(error[1])))\n",
    "            elif error[3] == 'Added':\n",
    "                sorted_data_full.append((error, idx.index(error[2])))\n",
    "        else:\n",
    "            sorted_data_full.append((error, idx.index(error[2])))\n",
    "    \n",
    "    sorted_data_full.sort(key=lambda x: x[1])\n",
    "    sorted_data_full = [item[0] for item in sorted_data_full]\n",
    "    sorted_data_full_err = \"{\" + \"}, {\".join(\", \".join(str(i) for i in item) for item in sorted_data_full) + \"}\"\n",
    "\n",
    "\n",
    "    if replaced_words:\n",
    "        full_mistakes +=  total_replaced_words\n",
    "    \n",
    "    if missed_words:\n",
    "        full_mistakes += total_missed_words_count\n",
    "\n",
    "    if added_words:\n",
    "        full_mistakes += total_count_of_added_words\n",
    "        \n",
    "    H_ERROR_CNT = half_mistakes\n",
    "    F_ERROR_CNT = full_mistakes\n",
    "    \n",
    "    if misspelled:\n",
    "        half_mistakes += total_spelling_mistakes\n",
    "            \n",
    "    sorted_data, result8 = find_fullstop_errors(ofile1_pth, ofile2_pth)\n",
    "    if sorted_data:\n",
    "        half_mistakes += len(sorted_data)\n",
    "        \n",
    "\n",
    "    with open(file2_path, 'r', encoding = 'utf-8') as file2:\n",
    "        words_file2 = file2.read().lower().split()\n",
    "        words_file2 = [word.strip(string.punctuation) for word in words_file2]\n",
    "        total_words = len(words_file2)\n",
    "\n",
    "    punctuation_set = set(string.punctuation)\n",
    "    punctuation_count = 0\n",
    "    tab_count = 0\n",
    "\n",
    "    with open(file2_path, 'r', encoding = 'utf-8') as file:\n",
    "        content = file.read()\n",
    "        for char in content:\n",
    "            if char in punctuation_set:\n",
    "                punctuation_count += 1\n",
    "\n",
    "        for line in file:\n",
    "            tab_count += line.count('\\t')\n",
    "                    \n",
    "\n",
    "    total_words_master = total_words + punctuation_count + tab_count\n",
    "    total_mistakes = (half_mistakes / 2) + full_mistakes \n",
    "    mistake_percentage = ((total_mistakes * 100) / 800)\n",
    "    \n",
    "    if mistake_percentage > 100:\n",
    "        mistake_percentage = 100\n",
    "\n",
    "    path_to_wkhtmltopdf = r'C:\\Program Files\\wkhtmltopdf\\bin\\wkhtmltopdf.exe'\n",
    "\n",
    "    config = pdfkit.configuration(wkhtmltopdf=path_to_wkhtmltopdf)\n",
    "\n",
    "    # Define your result variables\n",
    "    missing_space = out1\n",
    "    transposed_errors = result4\n",
    "    splitted_words = result5\n",
    "    misspelled_words = final_spelling_mistakes\n",
    "    combined_full = sorted_data_full_err\n",
    "    fullstop_err = result8\n",
    "    roll_no = roll_no(file1_path)\n",
    "    words_count1 = count_words(file1_path)\n",
    "    \n",
    "\n",
    "    # Function to format error details as a string\n",
    "    def format_error_details(error_list):\n",
    "        if len(error_list)>0:\n",
    "            return ''.join(error_list)\n",
    "        else:\n",
    "            return ''\n",
    "\n",
    "    # Categorize errors\n",
    "    spelling_mistake = format_error_details(misspelled_words)\n",
    "    other_than_spelling_mistake = format_error_details(combined_full) \n",
    "    \n",
    "    # Calculate the counts for the other error types\n",
    "    spacing_cap_transp_mistake = format_error_details(result4) + format_error_details(out1) + format_error_details(splitted_words)\n",
    "\n",
    "    html_string = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html>\n",
    "    <head>\n",
    "    <meta charset=\"utf-8\">\n",
    "    <style>\n",
    "        table, th, td {{\n",
    "            border: 1px solid black;\n",
    "            border-collapse: collapse;\n",
    "            font-size: 16px;\n",
    "            font-family: 'Mangal', sans-serif;  /* Specify Mangal as the font family */\n",
    "        }}\n",
    "    </style>\n",
    "    </head>\n",
    "    <body  style=\"padding:15px;\">\n",
    "    <div style=\"text-align:center;\">\n",
    "    <h2 style=\"font-size: 32px;\">Staff Selection Commission</h2>\n",
    "    <h2 style=\"font-size: 32px;\">Typing Test Candidate Report</h2>\n",
    "    </div>\n",
    "\n",
    "    <div>\n",
    "    <p style=\"font-size: 20px;\"><b> Name:****** </b></p>\n",
    "    <p style=\"font-size: 20px;\"><b> Roll Number: {roll_no} </b></p>\n",
    "    <p style=\"font-size: 20px;\"><b> No. of words typed: {words_count1} </b></p>\n",
    "    </div>\n",
    "\n",
    "    <table style=\"width:100%\">\n",
    "    <tr> \n",
    "        <th colspan=\"2\" style=\"background-color: LightSteelBlue;\">Type of Mistakes</th>\n",
    "        <th style=\"background-color: LightSteelBlue;\">No. of Error</th>\n",
    "        <th style=\"background-color: LightSteelBlue;\">Error Detail</th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=\"1\"><strong> Full Mistake </strong></td>\n",
    "        <td>Other than Spelling Mistake (Omission/Substitution except transposition/ Addition/ Incomplete Word) </td>\n",
    "        <td> {(total_replaced_words) + (total_missed_words_count) + (total_count_of_added_words)} </td>\n",
    "        <td>{other_than_spelling_mistake}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td rowspan=\"3\"><strong> Half Mistake </strong></td>\n",
    "        <td>Splitted/Concatnated/Transposition Mistake</td>\n",
    "        <td>{len(out1) + len(result4) + len(splitted_words)}</td>\n",
    "        <td>{spacing_cap_transp_mistake}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Spelling Mistake</td>\n",
    "        <td>{total_spelling_mistakes}</td>\n",
    "        <td>{spelling_mistake}</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td>Punctuation Error</td>\n",
    "        <td>{len(sorted_data)}</td>\n",
    "        <td>{fullstop_err}</td>\n",
    "    </tr>\n",
    "    </table>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "\n",
    "    pdf_output_path = r\"C:\\Users\\AnshChoudhary\\Downloads\\Skilltest-2024\\Skilltest-2024\\Evaluated\\3rd Jan- S1\\PDF_Evaluated 3rd Jan- S1\\{}.pdf\".format(roll_no)\n",
    "\n",
    "    options = {\n",
    "        'page-size': 'A4',\n",
    "        'margin-top': '10mm',\n",
    "        'margin-right': '30mm',\n",
    "        'margin-bottom': '10mm',\n",
    "        'margin-left': '0mm',\n",
    "        'encoding': 'UTF-8',\n",
    "    }\n",
    "\n",
    "    pdfkit.from_string(html_string, pdf_output_path, configuration=config, options=options)\n",
    "\n",
    "    # print(\"PDF saved at:\", pdf_output_path)\n",
    "    # print(\"Total time taken:- \")\n",
    "    \n",
    "    return total_spelling_mistakes, H_ERROR_CNT, F_ERROR_CNT, total_mistakes, total_words, mistake_percentage, len(sorted_data)\n",
    "    \n",
    "def candidate_hindi_replacement(file1_path):\n",
    "    with open(file1_path, 'r', encoding='utf-8') as file1:\n",
    "        candidate_text = file1.read()\n",
    "    candidate_text = remove_commasandfullstop(candidate_text)\n",
    "    CwordList = candidate_text.split()\n",
    "    \n",
    "    replacements = {\n",
    "    'अत:': 'अतः',\n",
    "    'प्रारंभ': 'प्रारम्भ',\n",
    "    'आरम्भ': 'आरंम्भ', \n",
    "    'आरंभ': 'आरंम्भ',\n",
    "    'किन्तु': 'किंतु',\n",
    "    'बाँध': 'बांध',\n",
    "    'उत्तर-प्रदेश': 'उत्तर प्रदेश',\n",
    "    'चीनी': 'चीनीं',\n",
    "    'ले जाने': 'ले-जाने',\n",
    "    'शत-प्रतिशत': 'शतप्रतिशत',\n",
    "    'सुविधाएँ': 'सुविधाएं',\n",
    "    'सुविधाऐं': 'सुविधाएं',\n",
    "    'पन्द्रह': '15', \n",
    "    '१५': '15', \n",
    "    'पंद्रह': '15',\n",
    "    '2011': 'दो हजार ग्यारह',\n",
    "    '२०११': 'दो हजार ग्यारह',\n",
    "    'पँचायती': 'पंचायती',\n",
    "    'सँस्थानों': 'संस्थानों',\n",
    "    '2800': 'दो हजार आठ सौ',\n",
    "    '२८००': 'दो हजार आठ सौ',\n",
    "    'पँचायतों': 'पंचायतों',\n",
    "    'पन्द्रह': '15',\n",
    "    '१५': '15',\n",
    "    'खण्डों': 'खंडों',\n",
    "    'तीन': '3',\n",
    "    '३': '3',\n",
    "    'सम्पूर्ण': 'संपूर्ण',\n",
    "    'जन आंदोलन': 'जन-आंदोलन',\n",
    "    '2012': 'दो हजार बारह',\n",
    "    '२०१२': 'दो हजार बारह',\n",
    "    '1990': 'उन्नीस सौ नब्बे',\n",
    "    '१९९०': 'उन्नीस सौ नब्बे',\n",
    "    '2010': 'दो हजार दस',\n",
    "    '२०१०': 'दो हजार दस',\n",
    "    'एक दशमलव आठ': '1.8',\n",
    "    '१ दशमलव ८': '1.8',\n",
    "    '१.८': '1.8',\n",
    "    'दो दशमलव पांच': '2.5',\n",
    "    '२ दशमलव ५': '2.5',\n",
    "    '२.५': '2.5',\n",
    "    'दस': '10',\n",
    "    '१०': '10',\n",
    "    # 'चार': '4',\n",
    "    # '४': '4',\n",
    "    'आँकड़े': 'आंकड़े',\n",
    "    'सैंतालीस': '47',\n",
    "    '४७': '47',\n",
    "    '%': 'प्रतिशत',\n",
    "    'प्रति शत': 'प्रतिशत',\n",
    "    'प्रति-शत': 'प्रतिशत',\n",
    "    'ग्यारह': '11',\n",
    "    '११': '11',\n",
    "    'मानदंडों': 'मानदण्डों',\n",
    "    'गन्दा': 'गंदा',\n",
    "    'सुविधाऐं': 'सुविधाएं',\n",
    "    'गाँवों': 'गांवों',\n",
    "    'गाँव': 'गांव',\n",
    "    'सड़सठ': '67',\n",
    "    '६७': '67',\n",
    "    'छ:': 'छः',\n",
    "    # '6': 'छः',\n",
    "    'छह': 'छः',\n",
    "    '६': 'छः',\n",
    "    'सम्बधी': 'संबंधी',\n",
    "    'संबन्धी': 'संबंधी',\n",
    "    'सम्बन्धी': 'संबंधी',\n",
    "    'केन्द्रित': 'केंद्रित',\n",
    "    'आँगनबाड़ियों': 'आंगनबाड़ियों',\n",
    "    'शत-प्रतिशत': 'शतप्रतिशत',\n",
    "    'सन्देश': 'संदेश',\n",
    "    'जाएँगे': 'जाएंगे',\n",
    "    'उपलब्धियाँ': 'उपलब्धियां',\n",
    "    'स्तम्भ': 'स्तंभ',\n",
    "    'चाहूँगी': 'चाहूंगी',\n",
    "    'ढाँचों': 'ढांचों',\n",
    "    'अलग अलग': 'अलग-अलग',\n",
    "    'प्रौद्योगिकियों': 'प्रौद्योगिकियों',\n",
    "    'निरन्तर': 'निरंतर',\n",
    "    'ढँग': 'ढंग',\n",
    "    'पसँद': 'पसंद',\n",
    "    'हूँ': 'हूं',\n",
    "    'आन्दोलन': 'आंदोलन',\n",
    "    'प्रधान मंत्री': 'प्रधानमंत्री',\n",
    "    'दूँ': 'दूं',\n",
    "    'मुख्य मंत्री': 'मुख्यमंत्री',\n",
    "    'स्वतन्त्र': 'स्वतंत्र',\n",
    "    'स्वत्तंत्रता': 'स्वतंत्रता',\n",
    "    'सम्भव': 'संभव',\n",
    "    'सद्भाव': 'सद्भाव',\n",
    "    'औद्योगिक': 'औद्योगिक',\n",
    "    '600 करोड़': 'छः सौ करोड़',\n",
    "    '६०० करोड़': 'छः सौ करोड़',\n",
    "    '६ सौ करोड़': 'छः सौ करोड़',\n",
    "    'अन्दर': 'अंदर',\n",
    "    'बन्द': 'बंद',\n",
    "    'यहाँ': 'यहां',\n",
    "    'वहाँ': 'वहां',\n",
    "    'परन्तु': 'परंतु',\n",
    "    'साँसदों': 'सांसदों',\n",
    "    'उद्योग': 'उद्योग',\n",
    "    'बापूजी': 'बापू जी',\n",
    "    'गाँधीजी': 'गांधी जी',\n",
    "    'गांधीजी': 'गांधी जी',\n",
    "    'गाँधी जी': 'गांधी जी',\n",
    "    'गाँधी': 'गांधी'\n",
    "    }\n",
    "\n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'गाँधी' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'जी':\n",
    "            CwordList[idx] = 'गांधी जी'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "    \n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '6' and (idx + 1) < len(CwordList) and not(CwordList[idx + 1] == 'सौ' or CwordList[idx + 1] == 'सौं'):\n",
    "            CwordList[idx] = 'छः'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'चार' and (idx + 1) < len(CwordList) and not CwordList[idx + 1] == 'दीवारी':\n",
    "            CwordList[idx] = '4'\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '४' and (idx + 1) < len(CwordList) and not CwordList[idx + 1] == 'दीवारी':\n",
    "            CwordList[idx] = '4'\n",
    "\n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'ले' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'जाने':\n",
    "            CwordList[idx] = 'ले-जाने'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "        \n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'अलग' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'अलग':\n",
    "            CwordList[idx] = 'अलग-अलग'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "    \n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word in replacements:\n",
    "            CwordList[idx] = replacements[word]\n",
    "            \n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '२' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'दशमलव' and (idx +2) < len(CwordList) and CwordList[idx + 2] == '५':\n",
    "            CwordList[idx] = '2.5'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "            indexes_to_remove.append(idx + 2)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "    \n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'दो' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'दशमलव' and (idx +2) < len(CwordList) and CwordList[idx + 2] == 'पांच':\n",
    "            CwordList[idx] = '2.5'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "            indexes_to_remove.append(idx + 2)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "        \n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '१' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'दशमलव' and (idx +2) < len(CwordList) and CwordList[idx + 2] == '८':\n",
    "            CwordList[idx] = '1.8'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "            indexes_to_remove.append(idx + 2)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "        \n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'एक' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'दशमलव' and (idx +2) < len(CwordList) and CwordList[idx + 2] == 'आठ':\n",
    "            CwordList[idx] = '1.8'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "            indexes_to_remove.append(idx + 2)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "        \n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'जन' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'आंदोलन':\n",
    "            CwordList[idx] = 'जन-आंदोलन'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word in replacements:\n",
    "            CwordList[idx] = replacements[word]\n",
    "            \n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '600' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'करोड़':\n",
    "            CwordList[idx] = 'छः सौ करोड़'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "        \n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '६००' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'करोड़':\n",
    "            CwordList[idx] = 'छः सौ करोड़'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "\n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == '६' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'सौ' and (idx +2) < len(CwordList) and CwordList[idx + 2] == 'करोड़':\n",
    "            CwordList[idx] = 'छः सौ करोड़'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "            indexes_to_remove.append(idx + 2)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "\n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'छह' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'सौ' and (idx +2) < len(CwordList) and CwordList[idx + 2] == 'करोड़':\n",
    "            CwordList[idx] = 'छः सौ करोड़'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "            indexes_to_remove.append(idx + 2)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "        \n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'प्रधान' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'मंत्री':\n",
    "            CwordList[idx] = 'प्रधानमंत्री'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "\n",
    "    indexes_to_remove = []\n",
    "    for idx, word in enumerate(CwordList):\n",
    "        if word == 'मुख्य' and (idx + 1) < len(CwordList) and CwordList[idx + 1] == 'मंत्री':\n",
    "            CwordList[idx] = 'मुख्यमंत्री'\n",
    "            indexes_to_remove.append(idx + 1)\n",
    "\n",
    "    indexes_to_remove.sort(reverse=True)\n",
    "    for idx in indexes_to_remove:\n",
    "        CwordList.pop(idx)\n",
    "\n",
    "    Ctext = ' '.join(CwordList)\n",
    "    Ctext = Ctext.strip()\n",
    "    # print(Ctext)\n",
    "\n",
    "    return Ctext\n",
    "\n",
    "def main(main_pth):\n",
    "    start_time = time.time()\n",
    "    cols = ['asr_rollno', 'asr_region_code', 'asr_date_appeared', 'asr_batch_no', 'asr_spelling_mistakes', 'asr_half_error', 'asr_misc_error', 'asr_punctuation_error','asr_total_mistakes', 'asr_no_of_word_original', 'asr_per_of_error', 'asr_stenograde', 'asr_remarks']\n",
    "    ldc_error = pd.DataFrame(columns= cols)\n",
    "\n",
    "    main_folder = os.listdir(main_pth)\n",
    "    file1_pth = \"\"\n",
    "    file2_pth = \"\"\n",
    "    allowed_spellings_pth = \"\"\n",
    "    asr_remarks = \"Steno-Hindi\"\n",
    "    asr_stenograde = \"Grade-D\"\n",
    "    temp_root = r\"C:\\Users\\AnshChoudhary\\Downloads\\Skilltest-2024\\Skilltest-2024\\Evaluated\\3rd Jan- S1\\temp_text\"\n",
    "    \n",
    "    for root, dirs, files in os.walk(main_pth):\n",
    "        for file in files:\n",
    "            if file.endswith(\".csv\"):\n",
    "                allowed_spellings_pth = os.path.join(root, file)  \n",
    "                #print(allowed_spellings_pth)  \n",
    "\n",
    "            elif file.endswith(\".txt\"):\n",
    "                if file.lower().startswith(\"master\"):\n",
    "                    ofile2_pth = os.path.join(root, file)\n",
    "\n",
    "                    temp_file2 = candidate_hindi_replacement(ofile2_pth)\n",
    "                    temp_pth = os.path.join(temp_root,\"master.txt\") \n",
    "                    with open(temp_pth, 'w', encoding='utf-8') as file2:\n",
    "                        file2.write(temp_file2)\n",
    "                    \n",
    "                    file2_pth = temp_pth\n",
    "\n",
    "                else:\n",
    "                    ofile1_pth = os.path.join(root, file)\n",
    "                    \n",
    "                    path_lst = ofile1_pth.split('\\\\')\n",
    "                    f = path_lst[-1].split('.')\n",
    "                    asr_rollno = f[-2]\n",
    "                    first_digit = str(asr_rollno)[0]\n",
    "                    if first_digit == '1':\n",
    "                        asr_region_code = 'NWR'\n",
    "                    elif first_digit == '2':\n",
    "                        asr_region_code = 'NR'\n",
    "                    elif first_digit == '3':\n",
    "                        asr_region_code = 'CR'\n",
    "                    elif first_digit == '4':\n",
    "                        asr_region_code = 'ER'\n",
    "                    elif first_digit == '5':\n",
    "                        asr_region_code = 'NCR'\n",
    "                    elif first_digit == '6':\n",
    "                        asr_region_code = 'MPR'\n",
    "                    elif first_digit == '7':\n",
    "                        asr_region_code = 'WR'\n",
    "                    elif first_digit == '8':\n",
    "                        asr_region_code = 'SR'\n",
    "                    elif first_digit == '9':\n",
    "                        asr_region_code = 'KKR'\n",
    "                        \n",
    "                    asr_date_appeared = '03-01-2024'\n",
    "                    asr_batch_no =  'Shift-2'\n",
    "\n",
    "                    temp_file1 = candidate_hindi_replacement(ofile1_pth)\n",
    "                    temp_pth = os.path.join(temp_root,f\"{asr_rollno}.txt\") \n",
    "                    with open(temp_pth, 'w', encoding = 'utf-8') as file1:\n",
    "                        file1.write(temp_file1)\n",
    "\n",
    "                    file1_pth = temp_pth\n",
    "\n",
    "                    total_spelling_mistakes, H_ERROR_CNT, F_ERROR_CNT, total_mistakes, total_words, mistake_percentage,result8 = calculate_mistake_percentage(file1_pth, file2_pth, ofile1_pth, ofile2_pth) \n",
    "                    \n",
    "                    row = {'asr_rollno': asr_rollno,\n",
    "                           'asr_region_code': asr_region_code,\n",
    "                           'asr_date_appeared': asr_date_appeared,\n",
    "                           'asr_batch_no': asr_batch_no,\n",
    "                           'asr_remarks': asr_remarks,\n",
    "                           'asr_stenograde': asr_stenograde,\n",
    "                           'asr_spelling_mistakes': total_spelling_mistakes,\n",
    "                           'asr_half_error': H_ERROR_CNT,\n",
    "                           'asr_misc_error': F_ERROR_CNT,\n",
    "                           'asr_punctuation_error': result8,\n",
    "                           'asr_total_mistakes': total_mistakes,\n",
    "                           'asr_no_of_word_original': 800,\n",
    "                           'asr_per_of_error': mistake_percentage}\n",
    "\n",
    "                    ldc_error = pd.concat([ldc_error, pd.DataFrame([row])], ignore_index=True)\n",
    "\n",
    "    output_excel_path = r\"C:\\Users\\AnshChoudhary\\Downloads\\Skilltest-2024\\Skilltest-2024\\Evaluation\\3rd Jan- S1\\Percentage_report 3rd Jan- S1.xlsx\"\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    total_time = end_time - start_time\n",
    "    \n",
    "    \n",
    "    print(total_time)\n",
    "    ldc_error.to_excel(output_excel_path, index=False)\n",
    "\n",
    "main(r\"C:\\Users\\AnshChoudhary\\Downloads\\Skilltest-2024\\Skilltest-2024\\Evaluation\\3rd Jan- S1\") \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d52790c-45b6-4e20-b6b7-7dbe4672fb3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
